\chapter{Foundations of multi-agent reinforcement learning} \label{ch:marl}

\begin{chapter_outline}

This chapter provides a broader overview of reinforcement learning.
We first introduce reinforcement learning in Section~\ref{sec:ch2_Introduction} and define a stochastic game, the most common multi-agent framework, in Section~\ref{sec:ch2_stochastic_Game}.
Multi-agent reinforcement learning is commonly divided into three settings depending on the relative goals of agents as described in Section~\ref{sec:ch2_multi_agent_settings}.
However, a fourth particular setting exists where a single-agent learns, called single-agent reinforcement learning.
In Section~\ref{sec:ch2_single_agent_RL}, we provide essential details on this (\textbf{not so}) particular case where the foundations of multi-agent reinforcement learning approaches reside.
Finally, Section~\ref{sec:ch2_partial_observability} concludes this chapter with a discussion on partial-observability.

\end{chapter_outline}

\section{Introduction} 
\label{sec:ch2_Introduction}
Reinforcement learning is a machine learning setting to solve decision-making problems.
We hereafter paraphrase several reinforcement learning definitions:
\begin{enumerate}
\item Reinforcement learning is learning solutions of a sequential decision process by repeatedly interacting with an environment \citep{marl-book}.
%\item Reinforcement learning is a kind of ML where an agent has to learn how to interact with its environment. \citep{pml1Book}.
\item Reinforcement learning is a framework for sequential decision-making, one core topic of ML \citep{introDeepRL}.
\item Reinforcement learning is a method to solve problems where decisions are applied to a system over time to achieve a desired goal \citep{BusoniuErnstBook}.
\item Reinforcement learning is learning by interacting in its environment to maximise a numerical signal called reward \citep{sutton2018reinforcement}.
\end{enumerate}

From these definitions, we denote several keywords that guide any RL journey: environment, interaction, sequence, goal and reward.
But one is missing: agent.
Commonly, an agent is anything capable of acting upon information it perceives from its environment~\citep{russel2010}.
In RL, agent refers to any agent that learns by interacting with its environment, most commonly denominated as intelligent or artificial intelligent agents.
Trying to summarise all definitions, we obtain that RL agents interact with their environment by sequentially taking actions that will modify the environment and provide them with a reward, e.g., moving pieces at chess makes the board evolve positions until one wins or draws.

After defining the agent, we must also consider the number of agents interacting in the environment.
Single-agent reinforcement learning (SARL) becomes multi-agent reinforcement learning (MARL) when more than one agent learns in the same environment.
This chapter defines the general multi-agent reinforcement learning framework, followed by the three standard settings: cooperation, competition, and general sum.
We then provide an in-depth overview of single-agent reinforcement learning, intending to give enough background to the reader unfamiliar with RL.
Additionally, we discuss partial observability, an essential topic in RL because agents may not perceive their environment entirely.

This chapter aims to provide a broader overview of reinforcement learning and its main concepts.
We intentionally skip some interesting details, such as mathematical developments, demonstrations, or definitions.
However, we always refer the reader to references to go beyond our introductions.
We acknowledge that the background sections of this manuscript take a lot of inspiration from these cited works, especially from two of them: ``Reinforcement learning: An introduction'' \citep{sutton2018reinforcement}, well established in the community and ``Multi-Agent Reinforcement Learning: Foundations and Modern Approaches'' \citep{marl-book}, a recent book on the foundations of multi-agent reinforcement learning.
Finally, this manuscript focuses on RL approaches when considering agents interacting in an environment, but other approaches exist, and we refer to the book of \cite{russel2010} for many more.

\section{Stochastic game}
\label{sec:ch2_stochastic_Game}

% TODO check state space vs set of space

The stochastic game (SG) \citep{stochasticGames} is probably at the foundation of multi-agent reinforcement learning.
In an SG, a set of agents interact with the environment by observing the state of the environment, choosing actions and receiving rewards over a sequence of time, finite or not.
In this manuscript, we define a stochastic game by a tuple $[n, \mathcal{S}, \mathcal{U}, R, P, \gamma]$.
Any agent is represented by $a$, and the set of agents is $\mathcal{A} \equiv \{1,..,n\}$.
We thus define a specific agent as $a_i$ with $i \in \mathcal{A}$.

As presented in Figure \ref{fig:ch2_sg}, at each time step $t$, each agent $a$ selects an action $u_t^a \in \mathcal{U}^a$ based on the state of the environment $s_t \in \mathcal{S}$ with a probability given by its policy $\pi^a(u^a_t|s_t)$, where $\mathcal{S}$ is the state space, and $\mathcal{U}^a$ is the action space of agent $a$.
These $n$ selected actions form the joint action as $\mathbf{u} \in \mathcal{U}$, where $\mathcal{U} \equiv \bigtimes_{i \in \mathcal{A}} \mathcal{U}^{a_i}$ is the joint action space.
We also denote the joint policy by $\mathbf{\pi}$.
As a consequence of agents taking $\mathbf{u_t}$, the state of the environment $s_t$ transits to a new state $s_{t+1}$ with a probability $P(s_{t+1}, s_t, \mathbf{u_t})$ defined by the stochastic transition function $P:\mathcal{S} \times \mathcal{S} \times \mathcal{U} \rightarrow [0,1]$.

\begin{figure}
    \centering
    \includegraphics[width=.8\textwidth]{tex_thesis/figures/ch2/SG.pdf}
    \caption{Stochastic game.}
    \label{fig:ch2_sg}
\end{figure}

As the state transitions, each agent receives a reward denoted $r_t^{a_i} = R(s_{t+1}, s_t, \mathbf{u_t}, i)$ defined by the reward function $R: \mathcal{S} \times \mathcal{S} \times \mathcal{U} \times \mathcal{A} \rightarrow \mathbb{R}$.
The goal of each agent $a_i$ is to maximise its expected sum of discounted rewards $\mathbb{E}_{\mathbf{\pi}}\left[ \sum_{t=0}^{T-1} \gamma^t r^{a_i}_t \right]$, where $T$ is the time horizon and $\gamma \in ]0, 1]$ the discount factor.
In this thesis, the time horizon $T$ is considered finite, and it defines the length of an episode, which is a sequence of actions in the environment.
Intuitively, the discount factor $\gamma$ defines the importance of future reward.
Finally, we insist here that the reward received by an agent depends on the joint action taken and not only on its own.

Reinforcement learning is about finding or learning the policy that maximises the expected sum, which is called the optimal policy.
One solution to a stochastic game is finding a Nash equilibrium \citep{nash1950equilibrium}.
Such equilibrium is obtained when no agents are interested in changing their policy, meaning that if one changes its policy, its sum of discounted rewards will decrease.
In some games, several Nash equilibriums can exist, and one may provide a better sum of discounted rewards to all agents than another.
More details are provided in Chapter~\ref{ch:competition}.

Several comments can be addressed in the context of the works presented in this manuscript.
In the SG definition, some include an initial state $s_0$ distribution, which we keep implicit (e.g. in \citep{marl-book}).
While the state space can be composed of discrete or continuous variables, the action spaces are considered discrete in this manuscript.
Agents do not always observe the state of the environment entirely to select an action.
This partial observability is further developed in Section~\ref{sec:ch2_partial_observability}.
Finally, in the literature, an SG is sometimes called a Markov game (e.g. \citep{MarkovGames}).

\section{Multi-agent settings} 
\label{sec:ch2_multi_agent_settings}
The stochastic game definition proposed in Section~\ref{sec:ch2_stochastic_Game} provides a general framework for multi-agent systems.
However, three specific settings are commonly distinguished in the literature.
The first setting, cooperation, involves agents cooperating to achieve a common goal.
In the second setting, competition, agents compete to achieve an opposing goal.
The third setting is named general-sum and encompasses everything else.
It is thus convenient to adapt the definition of the stochastic game to these particular settings.
The difference in these settings is the relation between the reward functions of agents \citep{marl-book}.
If some agents cooperate, they could have a single reward shared by all agents, or if two agents compete, both can have an equal but opposite reward.
In this Section, we introduce these three settings and identify the two of interest in this manuscript: cooperation and mixed cooperation-competition.
Not much detail will be provided here as these settings are the concern of further chapters.
However, in the next Section~\ref{sec:ch2_single_agent_RL}, we develop more details about the particular setting: single-agent reinforcement learning.

\subsection{Cooperation} 
\label{sec:ch2_Cooperation}
When agents share the same goal, they cooperate, and it is possible to model it with a single reward function instead of a set of reward functions.
This is called "common reward games" in \citep{marl-book}.
Many problems can be considered as being a cooperative multi-agent setting.
Examples include robot coordination (e.g. in \citep{papoudakis2021benchmarking}), train scheduling (e.g., in \citep{mohanty2020flatland}), traffic control (e.g. in \citep{zhang2019cityflow}) but also games (e.g., Hanabi \citep{Bard_2020}).
\citet{oroojlooy2022review} provide a review of cooperative MARL, including a more detailed list of applications.

Part~\ref{part:coop} of this manuscript is dedicated to cooperative settings with a common reward.
In Chapter~\ref{ch:cooperation}, we provide the adaptation of the stochastic game definition to a cooperative setting, followed by examples of environments and methods to solve them.
Our contributions are presented in Chapter~\ref{ch:qvmix}, with a CTDE method, and in Chapter~\ref{ch:impmarl}, with a suite of environments.

\subsection{Competition} 
\label{sec:ch2_Competition}
When agents share opposite goals, they compete.
In competition, any action that benefits one agent incurs a retrofit to other ones.
This setting is also called a zero-sum game \citep{marl-book} because it is modelled such that all rewards sum to zero at any time.
In a two-agent zero-sum game, commonly known as a two-player zero-sum in the game theory literature \citep{russel2010}, a possible adaptation of the stochastic game is to have a single reward function.
One agent aims to maximise it while the other tries to minimise it.
We also refer to this setting as fully competitive.
A concrete example is chess with a single reward at the end of the game: $1$ if you won, $0$ if you lost or $1/2$ if you drew.
Other examples include card games (e.g. Poker \citep{poker}), board games (e.g. chess, shogi, and Go \citep{silver2018general} or Stratego \citep{stratego}) and video games (e.g. StarCraft II \citep{vinyals2019grandmaster})


\subsection{General sum} 
\label{sec:ch2_general_sum}
The third setting includes everything that is not fully cooperative or fully competitive.
It is impossible to apriori change the reward function of the general definition as agents' goals are not categorisable.
Maybe one of the best examples of the general sum setting is autonomous driving \citep{dinneweth2022multi}.
Another is trading, where multiple agents sell and buy on the market and must cooperate to achieve personal goals \citep{SHAVANDI2022118124}.

In this manuscript, we are interested in the mixed cooperative-competitive setting where two teams compete against each other.
We model it with only two reward functions, one per team.
Part~\ref{part:compet} is dedicated to this setting, and we study how methods designed for competition work alongside cooperation ones.
We provide background in Chapter~\ref{ch:competition}, including necessary background from the competitive literature, and then detail our contribution in Chapter~\ref{ch:2teams}.

\section{Single-agent reinforcement learning} 
\label{sec:ch2_single_agent_RL}
When there is single-agent learning in the environment, it is called Single-agent reinforcement learning (SARL).
The most significant part of the history of RL lies in this setting, especially when compared with multi-agent RL.
Indeed, the foundation of MARL relies on many works first proposed in the single-agent framework.
Hence, it serves as our starting point and forms the primary content of this background chapter before diving into MARL.
Specifically, in this Section, we introduce the Markov decision process and discuss some fundamentals of RL that are model-based against model-free, dynamic programming and value-based alongside policy-based methods.

\subsection{Markov decision process}
\label{sec:ch2_mdp}

The definition of a Markov decision process (MDP) is obtained by considering a single agent in a stochastic game (see Section~\ref{sec:ch2_stochastic_Game}).
We define an MDP as a tuple $[\mathcal{S}, \mathcal{U}, R, P, \gamma, p_0]$ where a single agent interacts with its environment as presented in Figure~\ref{fig:ch2_mdp}.
The agent observes the state $s_t \in \mathcal{S}$ and selects an action from its action space $u_t \in \mathcal{U}$ with a probability defined by its policy $\pi(u_t|s_t)$.
This selected action leads the agent to a new state $s_{t+1}$ with a probability given by the transition function $P:\mathcal{S} \times \mathcal{S} \times \mathcal{U} \rightarrow [0,1]$.
One must denote here the Markovian property: the next state is a function only of the current state and action.

Along the transition of the state, the agent receives a reward $r_t$ defined by the reward function $R:\mathcal{S} \times \mathcal{S} \times \mathcal{U} \rightarrow \mathbb{R}$.
We define the discounted return of a sequence from time step $t$ to time step $T$ as $G_t= \sum_{j=t}^{T-1} \gamma^j r_{t+j}$.
The goal of the agent is to maximise the expected discounted return, so the expected sum of discounted rewards, over a finite episode $\mathbb{E}_{\pi} \left[\sum_{t=0}^{T-1} \gamma^t r_t \right]$, also denoted $\mathbb{E}_{\pi, p_0}\left[ G_0 \right]$ where $p_0$ is the initial distribution of $s_0$.

\begin{figure}
    \centering
    \includegraphics[width=.8\linewidth]{tex_thesis/figures/ch2/MDP.pdf}
    \caption{Markov decision process \citep{sutton2018reinforcement}.}
    \label{fig:ch2_mdp}
\end{figure}


Therefore, it needs to learn an optimal policy $\pi^*=\argmax_\pi \mathbb{E}_{\pi}\left[ G_0 \right]$.
To evaluate a policy, we define the state value function $V^\pi(s) = \mathbb{E}_{\pi}\left[G_t|s_t=s\right]$ and the state action value function $Q^\pi(s, u) = \mathbb{E}_{\pi}\left[G_t|s_t=s, u_t=u\right]$.
While it may seem meaningless for a single-agent environment, this optimal policy is a Nash equilibrium.
Indeed, the agent can not profit from changing its policy.

Most commonly, two families of methods are identified in RL to learn this optimal policy.
The value-based methods learn a value function and derive the policy from it, while the policy-based methods directly learn a policy.
But before diving into these details, we first discuss whether the model is known or not.

\subsection{Model-based or model-free}
\label{sec:ch2_model_based_vs_model_free}

When finding the optimal policy in an MDP, we distinguish methods based on the knowledge of action outcomes.
Indeed, with the model of the MDP, one can simulate the environment to evaluate policies \citep{sutton2018reinforcement}.
\cite{moerland2023model} defines a model as follows: ``A model is a form of reversible access to the MDP dynamics (known or learned)''.
Knowing the model and finding the optimal solution, or learning the model to find the optimal solution of this model, is referred to as model-based RL.
In opposition, learning by trial and error, without a model, is named model-free RL, and in this manuscript, we focus on model-free RL.
Model-based RL is not the primary focus, and we refer to the work of \cite{moerland2023model} that provides many keys to bridge the gap between RL and planning, between model-based and model-free.
Nevertheless, the following Section introduces dynamic programming, techniques that require a model to compute the optimal policy.
Dynamic programming provides the foundation of model-free methods that will afterwards be introduced.

\subsection{Dynamic programming}
Dynamic programming \citep{bellman1966dynamic} methods compute the optimal policy given a model of an MDP \citep{sutton2018reinforcement}.
They rely on the property that the value functions $V^{\pi^*}$ and $Q^{\pi^*}$ of the optimal policy ${\pi^*}$ satisfy the Bellman equations $\forall s, u$:
\begin{equation}
\label{eq:ch2_bellmanV}
    V^{\pi^*}(s) = \max_u \mathbb{E}[r_t + \gamma V^{\pi^*}(s_{t+1})| s_t=s, u_t=u]
\end{equation}

\begin{equation}
\label{eq:ch2_bellmanQ}
    Q^{\pi^*}(s, u) = \mathbb{E}[r_t + \gamma \max_{u'} Q^{\pi^*}(s_{t+1}, u') |s_t=s, u_t=u]
\end{equation}

Not going into the details of policy evaluation, policy improvement, policy iteration and value iteration, e.g. presented in \citep{sutton2018reinforcement}, one can find the optimal policy by iteratively evaluating and improving the policy with the value functions.
This can be highlighted by developing the value functions for any policy $\pi$ and $\forall s, u$:
\begin{equation}
\label{eq:ch2_V_2}
\begin{split}
    V^\pi(s)= \mathbb{E}_{\pi}\left[G_t|s_t=s\right] & = \mathbb{E}_{\pi}\left[r_t + \gamma V^\pi(s_{t+1})|s_t=s\right]\\
     & = \sum_{u} \pi(u|s) \sum_{s'} P(s', s, u) (R(s', s, u) + \gamma V^\pi(s'))
\end{split}
\end{equation}

\begin{equation}
\label{eq:ch2_Q_2}
\begin{split}
    Q^\pi(s, u) = \mathbb{E}_{\pi}\left[G_t|s_t=s, u_t=u\right] & = \mathbb{E}_{\pi}\left[r_t + \gamma V^\pi(s_{t+1})|s_t=s, u_t=u \right] \\
    &  = \sum_{s'} P(s', s, u) (R(s', s, u) + \gamma V^\pi(s'))
\end{split}
\end{equation}

In addition to requiring the model knowledge $P$ and $R$, the complexity of these methods increases with the size of the state space, often referred to as the curse of dimensionality, which may limit their usage.
Because of our direction towards model-free RL, without the model knowledge, our interest here is to present Equations~\ref{eq:ch2_bellmanV},~\ref{eq:ch2_bellmanQ},~\ref{eq:ch2_V_2} and~\ref{eq:ch2_Q_2} that may ease the understanding of the learning concepts.


\subsection{Value-based methods} \label{sec:ch2_value_based_methods}
Value-based methods are designed to learn value functions.
Maybe one of the first methods in model-free RL is Q-learning \citep{watkins1992q}, where the state-action value function learned is the optimal one defined as $Q^{\pi^*}(s, u)=\max_{\pi}Q^\pi(s, u)$.
This enables the agent to greedily select the action $\pi^*(u|s)=\argmax_u Q^{\pi^*}(s, u)$.

Q-learning is classified as a tabular method because it maintains estimations of $Q(s, u)$ in a table, one for each state-action pair.
It updates these estimations based on themselves, also called bootstrapping and is based on temporal difference (TD) learning.
Following the update rule of Equation~\ref{eq:ch2_QLearning}, one can repeatedly update the estimation $Q(s, u)$ by adding the temporal difference weighted by a factor $\alpha$ controlling the update size.

\begin{equation}
\label{eq:ch2_QLearning}
    Q(s_t, u_t) \leftarrow Q(s_t, u_t) + \alpha \left[ r_t + \gamma \max_u Q(s_{t+1}, u) - Q(s_t, u_t) \right]
\end{equation}

It is important to denote that this algorithm allows to approximate $Q^{\pi^*}(s, u)$ indepentely of the policy used to sample transitions ($s_t, u_t, r_t, s_{t+1}$).
These transition samples are typically generated with an $\epsilon$-greedy policy, that takes a random action instead of the greedy one with a probability $\epsilon$.
This is a characteristic of the off-policy methods, whereas the on-policy methods improves the policy used to generate the transitions.
In this manuscript, we consider only off-policy value-based methods but on-policy methods are discussed further in Section~\ref{sec:ch2_Cooperation}.
To cite one, SARSA is a well-known on-policy value-based method, e.g. in \citep{sutton2018reinforcement}.

Despite being independent of the policy, this iterative process highlight the exploration-exploitation dillema in RL.
Either you only play the action that maximises your currently learned value function, and therefore you exploit, either you play a different action and you explore the possible outcomes.

With Q-learning, the table size to maintain increases as the state-action space size increases.
Therefore, it can become impractical to compute $Q(s, u)$ for each state-action pair, and it requires function approximators.
There exist various function approximators, but in this manuscript, we restrict ourselves to neural networks.

A neural network is a function $f: \mathcal{X} \rightarrow \mathcal{Y}$ that maps an input ($\in\mathcal{X}$) to an output ($\in\mathcal{Y}$) based on its parameters $\theta$: $y = f(x;\theta)$.
These parameters define a sequence of functions, linear or not.
These functions are differentiable, allowing optimising their parameters by following the gradient of an objective, commonly called a loss function $\mathcal{L}(\theta)$.
To minimise the loss, parameters can be updated by gradient descent: $\theta = \theta - \alpha \nabla \mathcal{L}(\theta)$.
Optimising a neural network is also referred to as training it.
There exist many loss functions to train neural networks, depending on the function to be approximated and nowadays, neural networks are very large, leading to the name of deep learning.
See \citep{zhang2023dive} or \citep{pml1Book} for many more details.
Finally, reinforcement learning with a neural network is called deep reinforcement learning, e.g. in \citep{introDeepRL}.
Hereafter, we introduce how to train such networks to approximate a value function and, in Section~\ref{sec:ch2_policy_based_methods}, to approximate a policy.

A standard method in RL, referred to as deep Q-network (DQN) \citep{Mnih2015}, is to approximate $Q(s, u)$ with a neural network $\theta$.
This can be achieved by minimising the loss defined in Equation~\ref{eq:ch2_dqnloss} where $B$ is the replay buffer and $\theta'$ is the target network.
The replay buffer $B$ stores transitions $\langle s_{t},u_{t},r_{t},s_{t+1}\rangle$ from which batches of transitions are sampled to update $\theta$ \citep{lin1992self}.
This replay buffer allows updating the neural network with past transitions.
In turns, the target network $\theta'$ is a copy of $\theta$ updated periodically that reduces the moving target problem as $\theta$ is updated several times before updating $\theta'$, e.g., in \citep{Mnih2015}.

\begin{equation}
\label{eq:ch2_dqnloss}
    \mathcal{L}(\theta) = \mathbb{E}_{\langle . \rangle\sim B} \big[\big(r_{t} + \gamma \max_u Q(s_{t+1}, u; \theta')- Q(s_{t}, u_{t}; \theta)\big)^{2}\big]
\end{equation}

In both Equations~\ref{eq:ch2_QLearning} and~\ref{eq:ch2_dqnloss}, the $max$ operator can introduce some positive bias. 
To overcome this bias, a method called Double Q-learning \citep{hasselt2010double}, and adapted to Q-learning with approximators \citep{van2016deep}, consists in selecting the action that maximises the updated $Q(., \theta)$ to compute the target state-action value.
The loss is therefore adapted as in Equation~\ref{eq:ch2_doubleQ}.

\begin{equation}
    \label{eq:ch2_doubleQ}
    \mathcal{L}(\theta) = \mathbb{E}_{\langle . \rangle\sim B} \big[\big(r_{t} + \gamma Q(s_{t+1}, \argmax_u Q(s_{t+1}, u;\theta) ; \theta')- Q(s_{t}, u_{t}; \theta)\big)^{2}\big]
\end{equation}

Double Q-learning is one of the possible improvements of DQN, and we refer to the Rainbow paper \citep{hessel2018rainbow} that addresses several others.
To cite one, the extension to distributional RL, which approximates distributions instead of expected returns, can be of interest \citep{bellemare2017distributional, THEATE2023199}. 

\subsection{Policy-based methods} \label{sec:ch2_policy_based_methods}
Policy-based methods are designed to learn the policy.
In this manuscript, we restrict to the subclass of policy gradient methods where a neural network parametrised by $\theta$ approximates a differentiable policy $\pi_\theta=\pi(u|s;\theta)$.
Policy gradient methods hence update $\theta$ to find the optimal policy that maximises the expected return denoted as  $J(\pi_\theta) = \mathbb{E}_{\pi_\theta}[G_0]$.
Maybe one of the first method is REINFORCE \citep{williams1992simple} which updates $\theta$  by ascending the gradient given in Equation~\ref{eq:ch2_reinforce_grad}: $\theta = \theta + \alpha \nabla_\theta J(\pi_\theta)$.

\begin{equation}
\label{eq:ch2_reinforce_grad}
    \nabla_\theta J(\pi_\theta) = \nabla_\theta \mathbb{E}_{\pi_\theta}[G_0] = \mathbb{E}\left[\sum_{t=0}^{T-1} Q(s_t, u_t) \nabla_\theta \log \pi(u_t|s_t;\theta)\right]
\end{equation}

Estimating $Q(s_t, u_t)$ instead of computing it is a solution proposed by the actor-critic methods \citep{sutton1999policy,konda1999actor}.
This type of method expands upon REINFORCE by incorporating a second neural network, called the critic and denoted by $\phi$, that estimates $Q(s_t, u_t;\phi)$ while the actor is the parametrised policy $\pi(u|s;\theta)$.
The new loss provided by incorporating the critic is given in Equation~\ref{eq:ch2_reinforce_grad}.
\begin{equation}
\label{eq:ch2_Q_actor_crit}
    \nabla_\theta J(\pi_\theta) = \mathbb{E}\left[\sum_{t=0}^{T-1} Q(s_t, u_t;\phi) \nabla_\theta \log \pi(u_t|s_t;\theta)\right]
\end{equation}

Moreover, a baseline can be injected into the gradient to reduce variance.
Usually, the baseline is  $V(s)$, independent of the action taken, and $Q(s, u;\phi)$ is replaced by the advantage function $A(s,u; \phi)$ \citep{10.5555/2074022.2074088}, leading to the gradient expressions of Equation~\ref{eq:ch2_baseline_actor_crit}:

\begin{align}
\begin{split}
\label{eq:ch2_baseline_actor_crit}
    \nabla_\theta J(\pi_\theta)
    & = \mathbb{E}\left[\sum_{t=0}^{T-1} [Q(s_t, u_t) - V(s_t)] \nabla_\theta \log \pi(u_t|s_t;\theta)\right]\\
    & = \mathbb{E} \left[\sum_{t=0}^{T-1} A(s_t, u_t; \phi) \nabla_\theta \pi(s_t, u_t; \theta)\right]
\end{split}
\end{align}

Estimating the advantage with only one neural network is possible either by $A(s_t,u_t; \phi)=Q(s_t, u_t;\phi)-\sum_u \pi(u|s_t;\theta) Q(s_t,u; \phi)$ or by $A(s_t,u_t; \phi)=r_t +\gamma V(s_{t+1};\phi) - V(s_t;\phi)$.
This critic can be trained on-policy or off-policy following methods described in Section~\ref{sec:ch2_value_based_methods}.
This is why actor-critic methods are sometimes described as a mix between value-based and policy-based methods.

Nowadays, advanced policy-based methods relying on the actor-critic paradigm appear to be the most successful.
We can cite trust region policy optimisation (TRPO) \citep{schulman2015trust} and its variant proximal policy optimisation (PPO) \citep{schulman2017ppo}.
Both methods rely on a controlled policy update by constraining the loss of REINFORCE defined in Equation~\ref{eq:ch2_reinforce_grad}.

\section{Partial observability} \label{sec:ch2_partial_observability}
As defined in previous Sections, the Markov decision process and the stochastic game are fully observable.
Agents have complete access to the state environment and perceive everything without uncertainty.
In real-world applications, it is not always possible to consider this feasible.
Anyone can come up with ideas of a partially observable environment.
Especially given our definition of agents ``acting upon information it perceives''.

Starting with SARL the Markov decision process is said to be a partially observable MDP (POMDP) \citep{KAELBLING199899} when the agent has only access to incomplete information about the state.
The MDP definition of Section~\ref{sec:ch2_mdp} is adapted easily to the POMDP by adding an observation space $\mathcal{Z}$ and an observation function $O:\mathcal{S} \times \mathcal{Z} \rightarrow [0, 1]$, mapping a state and an observation to the probability of observing the latest.
The corresponding execution diagram of a POMDP is presented in Figure \ref{fig:ch2_pomdp}.

\begin{figure}
    \centering
    \includegraphics[width=.8\linewidth]{tex_thesis/figures/ch2/POMDP.pdf}
    \caption{Partially observable Markov decision process.}
    \label{fig:ch2_pomdp}
\end{figure}


Consequently, the agent's policy $\pi$ cannot be a function of the state anymore.
One solution is to define the policy as a function of the observation.
However, it can be a bad idea because the observation does not respect the Markovian property defined in Section~\ref{sec:ch2_mdp}.
In POMDP, the policy is commonly a function of the history of past observations and actions $(\mathcal{Z} \times \mathcal{U})^\tau$, denoted as $\tau$.
We therefore denote the policy as $\pi(u_t|\tau_t,o_t): (\mathcal{Z} \times \mathcal{U})^t \rightarrow [0,1]$.
The corresponding $Q$ function is denoted $Q(\tau,u)$ instead of $Q(\tau,o, u)$.

In practice, history can become very large, which can be impractical.
A solution is to maintain a belief $b(s)=Pr(s|\tau,o)$, the probability of being in a given state, knowing the history of observations and actions.
To approximate this belief, it is possible to benefit from recurrent neural networks (RNN), such as GRU \citep{Chung2014EmpiricalModeling} or LSTM \citep{Hochreiter1997LongMemory}.
These networks compute a hidden state, updated at each timestep, which can be considered as a memory.
Recurrent networks have many applications, and in POMDP, their hidden state allows maintaining a belief without processing the whole history at each timestep.

Using recurrent networks to compute policies is thus a common practice in POMDP, resulting in recurrent policies.
Adding RNN has demonstrated convincing results, such as in recurrent policy gradients \citep{wierstra2010recurrent} or in deep recurrent Q-network (DRQN) \citep{Hausknecht2015DeepMDPs}.
We suggest readers interested in details read the pedagogical paper of \cite{lambrechts2022recurrent}.
This paper demonstrates that the correlation between the hidden state of RNNs, used to approximate policies, and the belief increases as the training progresses.

Adding partial observability in the definition of the stochastic game leads to the most general framework of multi-agent reinforcement learning, the partially observable stochastic game (POSG) \citep{hansen2004dynamic}.
Its definition is obtained by adding a set of $n$ observation spaces $\mathcal{Z}$ and a set of $n$ observation function $O$.
When agents are the same, they observe the state in the same, and a shortcut taken in the literature is to consider that these two sets are singleton.

However, as detailed in \citep{DecPomdp}, the belief cannot be considered similarly in multi-agent systems as in POMDP.
Being in a given state is not only a function of one agent's history but is a function of all agents' history.
Even when agents can observe the state totally, they may not observe the actions previously taken by others.
This manuscript does not address multi-agent belief, and we refer to \citep{DecPomdp} for more details.
In a POSG, we consider that an agent's policy is a function $\pi^{a}(u_t^{a}|\tau_t^{a},o_t^{a}): (\mathcal{Z}^a \times \mathcal{U}^a)^t \rightarrow [0,1]$, which maps its history $\tau_t^{a} \in (\mathcal{Z}^a \times \mathcal{U}^a)^{t-1}$ and its current observation $o_t^{a}$ to the probability of taking action $u_t^{a}$.
As in SARL, such a policy is commonly a recurrent policy approximated with RNN.


\todo{check que les exposant a ou subscript a sont les mÃªmes partout.}