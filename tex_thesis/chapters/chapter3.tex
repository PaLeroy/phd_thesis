\chapter{Cooperation}\label{ch:cooperation}
\begin{chapter_outline}

This chapter introduces the main concepts of learning to cooperate with multi-agent reinforcement learning.
While chapters~\ref{ch:qvmix} and~\ref{ch:impmarl} present our contributions in this field, this chapter covers the related works in the literature.
After a small introduction in Section~\ref{sec:ch3_intro}, we define the decentralised POMDP, an adaptation of the stochastic game to the cooperative setting in Section~\ref{sec:ch3_decpomdp}.
Once defined, we present examples of Dec-POMDP in Section~\ref{sec:ch3_env}, followed by a more precise description of the StarCraft Multi-agent challenge, an environment suite of importance in this manuscript.
We then present methods designed to solve such environments, respectively value-based methods in Section~\ref{sec:ch3_value} and policy-based methods in Section~\ref{sec:ch3_policy}.
For both families, we detail methods used in the contributions of this manuscript and list others from the literature.

\end{chapter_outline}

\damien{13/02}
\section{Introduction}
\label{sec:ch3_intro}
As introduced in Part~\ref{part:background}, cooperation is the multi-agent setting of agents sharing a common goal.
This manuscript considers the decentralised POMDP (Dec-POMDP) \citep{DecPomdp}, a framework where all agents receive the same reward.
Its definition, provided in Section~\ref{sec:ch3_decpomdp}, can be easily derived from the partially observable stochastic game one (see Section~\ref{sec:ch2_partial_observability}) by having a single reward.
Such a framework is also called common reward games, e.g. in \cite{marl-book}.

Nevertheless, when it comes to cooperation in a multi-agent system, one notion to discuss is whether the action selection and the training of agents are centralised or decentralised.
The difference lies in the information shared or not by agents.
This is referred to, in \citep{marl-book}, as the modes of execution and training.
This manuscript considers three combinations, hereafter described: centralised training and execution, decentralised training and execution, and centralised training with decentralised execution.

Since all agents share a common goal, we could centrally select all actions, sharing common knowledge.
This is the centralised mode, where agents are commonly trained with single-agent reinforcement learning methods.
For example, controlling a robotic hand composed of several actuators can be done with a single agent controlling every actuator.
However, as presented in Section~\ref{sec:ch2_partial_observability}, all agents may not access the same information or may perceive only a part of the state of the environment.
It would then be impossible, except in a simulated environment, to consider that agents can easily select actions in a centralised mode, with information not available to every one of them.

On the contrary, the decentralised execution mode is the one that considers that each agent has only access to a subset of the information, the one it perceives, and uses it to select its action.
The corresponding decentralised training mode is when these agents are trained based only on this perceived information.
We thus refer to the decentralised mode when training agents independently with single-agent reinforcement learning.
In other words, we assume they are the single agent learning in the environment.

Finally, the third mode unifies the two former and is the centralised training with decentralised execution (CTDE).
It allows decentralised execution, with each agent selecting actions based on their observations, but allows for the exploitation of more information during training.
Indeed, RL agents are usually trained in a simulator, having access to every piece of information.
Therefore, using the state of the environment or the actions made by other agents is commonly possible during training.

This Part~\ref{part:coop} focuses on methods that exploit the CTDE mode to solve cooperative tasks.
This chapter presents Dec-POMDP environments and CTDE methods issued from the literature. 
In contrast, the following chapters present our contributions: another CTDE method in Chapter~\ref{ch:qvmix} and a real-world application of Dec-POMDP in Chapter~\ref{ch:impmarl}.


\section{Decentralised partially observable Markov decision process}
\label{sec:ch3_decpomdp}
As introduced, the definition of the decentralised partially observable Markov decision process (Dec-POMDP) \citep{DecPomdp} can be derived from the stochastic game definition presented in Section~\ref{sec:ch2_stochastic_Game}.
It is substantially the same, but the reward function maps only to a single real, a reward common to all agents.

We define the Dec-POMDP by a tuple $[\mathcal{S}, \mathcal{Z}, \mathcal{U}, n, O, R, P, \gamma]$, where $n$ agents $a_i$, $i \in \mathcal{A} \equiv \{1,..,n\}$, simultaneously choose an action at every time step $t$.
The state of the environment is $s_t \in \mathcal{S}$ where $\mathcal{S}$ is the set of states.
The observation function $O: \mathcal{S} \times \mathcal{A} \rightarrow \mathcal{Z}$ maps the state to an observation $o_t^{a} \in \mathcal{Z}$ perceived by agent $a$ at time $t$, where $\mathcal{Z}$ is the observation space.
Each agent $a_i$ selects an action $u_t^{a_i} \in \mathcal{U}^{a_i}$, and the joint action space is $\mathcal{U}$.
After the joint action $\boldsymbol{u_t} \in \mathcal{U}$ is executed, the transition function determines the new state with probability $P(s_{t+1}|s_t, \boldsymbol{u_t}): \mathcal{S} \times \mathcal{S} \times\mathcal{U} \rightarrow  [0,1] $, and $r_t=R(s_{t+1}, s_t, \boldsymbol {u_t}): \mathcal{S} \times \mathcal{S} \times \mathcal{U} \rightarrow \mathbb{R}$ is the team reward obtained by all agents.

An agent's policy is a function $\pi^{a}(u_t^{a}|\tau_t^{a},o_t^{a}): (\mathcal{Z} \times \mathcal{U}^a)^t \rightarrow [0,1]$, which maps its history $\tau_t^{a} \in (\mathcal{Z} \times \mathcal{U}_a)^{t-1}$ and its observation $o_t^{a}$ to the probability of taking action $u_t^{a}$. 
The joint or team policy is denoted by $\boldsymbol{\pi}=(\pi^1,..,\pi^n)$.
The cumulative discounted reward obtained from time step $t$ over the next $T$ time steps is defined by $G_{t} = \sum_{k=0}^{T-1} \gamma^k r_{t+k}$ and $\gamma \in [0, 1)$ is the discount factor.
The goal of agents is to find the optimal joint policy that maximises the expected $G_t$ during the entire episode: $\boldsymbol{\pi^{*}} =\argmax_{\boldsymbol{\pi}} \mathbb{E}[G_0|\boldsymbol{\pi}]$.
Again, we emphasise that this expected sum depends on the joint policy.


\todo{oliehoek: individually observable and jointly observable}

\todo{check concordance with notation of stochastic game}

\todo{Challenges in Dec-POMDP + solved for what already? cf decpomdp book}

\section{Environments}
\label{sec:ch3_env}
Before diving into the details of CTDE methods, we provide an overview of the existing suites\footnotemark of environments from the literature.
We then describe the StarCraft multi-agent challenge (SMAC), perhaps one of the most studied environment suites in the community, but also used in our experiments in Chapter~\ref{ch:qvmix}.
Moreover, Chapter~\ref{ch:impmarl} is dedicated to a suite of environments, and we leave its description there.

\footnotetext{In this manuscript, an environment refers to an instance of a particular framework (here, of a Dec-POMDP). In the literature, we commonly see it to refer to either a collection of environments or an instance of one. We use "suite" to distinguish between these two.}

The multi-agent particle environments (MPE) \citep{lowe2017multi} is a popular suite in the MARL community with cooperative scenarios.
In MPE, particles move in a 2D grid with continuous action and state spaces.
This suite comes with all types of multi-agent settings: competition, cooperation, with and without competition.
A second environment with continual action space is MaMuJoCo \citep{peng2021facmac}.
MuJoCo \citep{todorov2012mujoco} stands for multi-joint dynamics with contact.
It is a famous physics-based simulator to learn to control, for example, a humanoid to run.
MaMuJoCo essentially factorises MuJoCo's decisions by decentralising the decision, typically having a different agent for each leg.
It is an example of a SARL environment extended to MARL.
Other cooperative environments based on game simulators include the Hanabi Challenge \citep{Bard_2020}, a "cooperative solitaire" between two and five players, and Google Research Football \citep{kurach2020google}, a football game simulator.

Cooperative MARL methods are mostly benchmarked on these games and simulators, but real-world applications also exist.
City flow \citep{zhang2019cityflow} are traffic control environments where agents control traffic lights in a city.
In Flatland \citep{mohanty2020flatland}, agents control trains to solve a scheduling problem, avoiding collisions by coordinating not to take the same routes.
The multi-robot warehouse \citep{papoudakis2021benchmarking, christianos2020shared} simulates warehouse agents needing to deliver requested goods.
Multi-Agent Tracking Environment (MATE) \citep{NEURIPS2022_b2a1c152} is a target coverage control problem where cameras are controlled to detect all targets.
A mixed cooperative-competitive game can be made by controlling the targets.
Many more examples of cooperative MARL applications exist.
Many can be found in the book of \cite{marl-book}, in the review on cooperative MARL done by \cite{oroojlooy2022review}, such as autonomous driving, resource allocation, stock market,...


\subsection{StarCraft multi-agent challenge} \label{sec:ch3_smac}
The StarCraft multi-agent challenge (SMAC) \citep{samvelyan2019starcraft} and its improved version SMACv2 \citep{ellis2023smacv2} are probably the most studied suite of environments with CTDE methods.
SMAC is based on the StarCraft II Learning Environment \citep{vinyals2017starcraft}, an RL environment to play StarCraft II (SC2).
StarCraft is a strategy video game where players compete by managing units, gathering resources, building an army and defeating opponents.
In such games, micro-management refers to unit management, unlike macro-management, which involves resource management.
Unlike playing the real game of StraCraft like in AlphaStar \citep{vinyals2019grandmaster}, SMAC is a suite of micro-management challenges where each game unit is an agent.
Many scenarios exist in SMAC, also called "maps", and all involve training a team to achieve a common goal.
Still, this common goal is to defeat an adversarial team controlled by the game's deterministic and stationary policy built-in AI.
In Figure~\ref{fig:ch3_smac}, we present the initial configurations of two maps where two teams face each other.
Also note that in Chapter~\ref{ch:2teams}, we present a contribution where SMAC has been modified to train both competing teams.

\begin{figure}
     \centering
     \begin{subfigure}[b]{0.5\textwidth}
         \centering
         \includegraphics[height=2.3cm]{tex_thesis/figures/ch3/3m_screen.pdf}
         \caption{3m}
         \label{fig:ch3_3m}
     \end{subfigure}%
     \begin{subfigure}[b]{0.5\textwidth}
         \centering
         \includegraphics[height=2.3cm]{tex_thesis/figures/ch3/3s5z_screen.pdf}
         \caption{3s5z}
         \label{fig:ch3_3s5z}
     \end{subfigure}
    \caption{Two scenarios of the StarCraft multi-agent challenges.}
    \label{fig:ch3_smac}
\end{figure}

We hereafter toughly define the elements of the Dec-POMDP implemented in SMAC.
Agents have partial observability defined by a sight range, a circle inside which they can observe other agents.
There are different components in the observation.
An agent observes information about itself: its remaining hit points and shield points and four booleans representing the direction it can move in (NSWE). 
It also observes information about other agents within its sight range: the relative distance, relative x, relative y and the remaining hit points and shield points. 
If the other agent is an ally, it also observes its last performed action performed.
When the other agent is an enemy, it observes if this agent is within shooting range.
The shooting range is smaller than the sight range and depends on the unit type, and some units even attack in melee.

The state of the environment accumulates all information from agents' observations.
As described, an agent perceives other agents with distances relative to itself and does not know where it is on the map.
In the state, the agents' positions are encoded with their coordinates relative to the centre of the map.'

Within all scenarios, the agent has the choice of eight actions: do nothing, move in one of four directions (NSWE) or attack one of its three opponents.
Some actions are forbidden in SMAC, such as an attack if the opponent is not within shooting range.
Therefore, agents must consider which ones are available before choosing an action.

At each timestep, the agent receives a zero or positive reward, common to each agent of the same team. 
This reward is a sum of three factors: a zero or positive reward for the damage dealt, a positive reward if an enemy unit's hit points reach zero, and a positive reward if all enemy units are defeated. 
Maximising the reward forces the team to neutralise every unit of the opposing team.

We hereafter describe both scenarios of Figure~\ref{fig:ch3_smac}.
There are other types of units in other scenarios, and we refer the reader to \citep{samvelyan2019starcraft}.
In the $3m$ scenario, represented in Figure~\ref{fig:ch3_3m}, six marines compete in two teams of three.
A marine has $45$ hit points and shoots at range, inflicting $6$ damage points to an opponent for each attack.
In the $3s5z$ scenario, represented in Figure~\ref{fig:ch3_3s5z}, six stalkers and ten zealots compete in two teams of eight.
Both units have shield points in addition to hit points.
A shield receives a different amount of damage and regenerates over time if the unit is not attacked again for a given time.
A stalker has $80$ hit points and $80$ shield points.
It shoots at range, inflicting $13$ damage points to the shield, $12$ damage points to a zealot's hit points and $17$ to a stalker's hit points.
A zealot has 100 hits points and 50 shield points.
It attacks in melee and inflicts $16$ damage points to the shield and $14$ damage points to the hit points of a zealot or a stalker.
All maps are presented in a video from the author\footnote{\url{https://youtu.be/VZ7zmQ_obZ0}}.

Finally, it can be intriguing to see an environment where two teams compete is considered cooperative.
This is because the built-in AI is stationary. 
These agents are not learning agents and can be considered part of the environment.
The built-in AI strategy is a rules-based policy.
Specifically, each agent moves toward the starting point of the opponent's team until it reaches the opposite side of the map and stops.
If they encounter opponents in their sight range, they select one as their target based on a priority score.
They will choose to attack the closest unit with the highest priority, which will remain the target until its priority drops or until it can no longer be attacked.
A unit's priority score is based on its type and current action.
For example, if two of the same units attack and the targeted unit stops attacking, its priority score will drop, and the built-in AI agents will select the other unit to attack.
One of the weaknesses of this built-in AI strategy that the learning agents need to learn is to stop attacking to stay alive, with the condition that an ally should be attacking.
So, agents need to cooperate to attack or not share the possible damage inflicted by the opponents.
The next sections cover the MARL methods to achieve this coordination.

\section{Value-based methods}
\label{sec:ch3_value}

As detailed in Chapter \ref{ch:marl}, value-based methods aim to learn the optimal state-action value function $Q^{\pi^*}(s, u)$, such that the optimal policy is $\pi^*(s)=\argmax_{u} Q^{\pi^*}(s, u)$.
In SARL, one solution, called DQN, is to approximate $Q$ with a neural network $\theta$ and learn $Q(s, u;\theta)$
by minimising the loss defined in Equation~\ref{eq:ch2_dqnloss}.

As introduced in Section~\ref{sec:ch3_intro}, there exist different modes of training and execution.
In the first mode, one possible centralised training and execution approach is DQN to train a centralised learner in a Dec-POMDP.
However, it is only possible if all agents can access the state $s$ during execution.
Such a setting is also called a multi-agent Markov decision process \citep{boutilier1996planning}.
In this setting, the centralised agent learns the state-joint-action value function $Q^{\boldsymbol{\pi}}(s,\mathbf{u}; \theta)$ by following the adapted DQN loss in Equation~\ref{eq:ch3_centralQloss}.

\begin{equation}
\label{eq:ch3_centralQloss}
    \mathcal{L}(\theta) = \mathbb{E}_{\langle . \rangle\sim B} \big[\big(r_{t} + \gamma \max_{\mathbf{u}} Q(s_{t+1}, \mathbf{u}; \theta')- Q(s_{t}, \mathbf{\mathbf{u_t}}; \theta)\big)^{2}\big]
\end{equation}

Issues are that the joint action space scales exponentially with $n$.
Also, in practice, agents select their action based only on their history $(o, \tau)$ and not the state $s$.

For the second mode, the decentralised training and execution, each agent can learn its own Q-value independently $Q_a=Q^{\pi^a}(\tau^a, u^a)$, agnostically of the existence of other agents.
IQL \citep{Tan1993} is the extension of Q-Learning (see Chapter~\ref{ch:marl}) to this mode. 
The equivalent extension has been performed with DQN \citep{TampuuDqnIQL}, also called IQL.
Typically, in a Dec-POMDP, a recurrent network approximates the independent Q-value, as explained in Section \ref{sec:ch2_partial_observability}.
One problem with IQL is that agents must select actions which maximise $Q(s_t, \mathbf{u_t})$ while ignoring, at any time, actions taken by other agents.

This is where centralised training with decentralised execution (CTDE), the third mode, comes in handy.
With CTDE methods, it is possible to approximate $Q(s_t, \mathbf{u_t})$ as a factorisation of individual $Q_a$ functions during training such that $\mathbf{u_t}$ maximises both the joint and the individual $Q_a$ functions.
To ensure this, individual $Q_a$ functions must satisfy the individual-global-max condition (IGM) \citep{Son2019QTRAN:Learning} presented in Equation~\ref{eq:ch3_igm}.

\begin{equation}
    \argmax_{\mathbf{u_t}} Q(s_t, \mathbf{u_t}) =(\argmax_{u_t^{1}} Q_{1}(\tau_t^{1}, u_t^{1}),...,\argmax_{u_t^{n}} Q_{n}(\tau_t^{n}, u_t^{n}))
    \label{eq:ch3_igm}
\end{equation}

Satisfying the IGM property allows agents to greedily select actions that maximise their individual $Q_a$ and the state-joint-action value function $Q(s_t, \mathbf{u_t})$.
Therefore, during training, an approximation of $Q(s_t, \mathbf{u_t})$, sometimes denoted $Q_{tot}$, is built as a factorisation of $Q_a$ and then dropped at execution.
Agents select actions based on these $Q_a$, satisfying IGM, and we keep only these networks at execution.
Note that these $Q_a$ are now utility functions, only used to select actions, and they do not approximate the expected sum of discounted rewards anymore.
In this manuscript, we express $Q_{tot}$ as a function of the state $s$ and consider that the state is known.
The following expressions can be rewritten as a function of the joint history $\mathbf{\tau}$ if the state $s$ is unknown during training.

Maybe one of the first methods of value-based factorisation is called value decomposition network (VDN) \citep{sunehag2018vdn}.
This method factories $Q_{tot}$ using the addition, an operation which satisfies IGM: $Q_{tot}^{VDN}(s_t, \mathbf{u_t}) = \sum_{i=1}^n Q_{a_i}(\tau^{a_i}_t, u^{a_i}_t)$.
Leaving the details of the training procedure for later, one limit of VDN is that factorising through addition does not allow to approximate complex joint-action-state Q functions.
Another limitation is that we need $Q_{tot}$ only during training, and in VDN, we do not benefit from additional information at our disposal, such as the state.
In the following, we present three other methods that have been used in the experiment conducted in the presented contributions.
Others exist, and we finish this section with a related work discussion.

\subsection{QMIX}
QMIX \citep{Rashid2018} is a CTDE method where the approximation of $Q_{tot}$ is performed by a monotone factorisation of the individual $Q_a$ functions, also function of the state during training, as defined in Equation~\ref{eq:qmixappendix}.

\begin{equation}
     Q_{tot}^{mix}(s_t, \mathbf{u_t})=\text{Mixer} \left(Q_{a_1}(\tau^{a_1}_t, u_t^{a_1}) ,..,Q_{a_n}(\tau^{a_n}_t, u_t^{a_n}), s_t\right)
     \label{eq:qmixappendix}
\end{equation}

\begin{figure}
\centering
\input{tex_thesis/tikz/ch3/qmix}
\caption{QMIX architecture. Individual $Q_a$ are in red, and the mixing network is in blue.}
\label{fig:qmix}
\end{figure}

The monotonicity is ensured by a hypernetwork \citep{Ha2016HyperNetworks} $h_p(.): \mathcal{S} \rightarrow \mathbb{R}^{|\phi|+}$ which computes, from the state $s_t$, the parameters $\phi$ of a mixer network $h_m(.)$.
To ensure monotonicity, the weights defined by $\phi$ are constrained to be positive.
This mixer network is $h_m(.): \mathbb{R}^n \times \phi \rightarrow \mathbb{R}$.
Together, $h_p(.)$ and $h_m(.)$ defines the mixer such that $ Q_{tot}^{mix}(s_t, \mathbf{u_t}) = h_m\left(Q_{a_1},..,Q_{a_n}, h_p(s_t)\right)$.
The monotonicity of $Q_{tot}^{mix}$ with respect to the individual $Q_a$ functions, defined in Equation \ref{eq:ch3_monotonicity}, is satisfied because a neural network comprised of monotonic functions ($h_m$) and strictly positive weights ($h_p$) is monotonic with respect to its inputs ($Q_a$). 
The entire QMIX architecture is presented in Figure~\ref{fig:qmix}.

\begin{equation}
    \frac{\partial Q_{tot}^{mix}(s_t, \mathbf{u_t})}{\partial Q_{a}(s_t, u_t^{a})} \geq 0 \text{ } \forall a
    \label{eq:ch3_monotonicity}
\end{equation}

\begin{figure}
    \centering
\input{tex_thesis/tikz/ch3/indivQ}
\caption{Common $Q_a$ network implementation. The hidden state $h$ embeds the history, and the action space size defines the number of outputs of the network.}
\label{fig:ch3_indivQ}
\end{figure}

Since the Dec-POMDP induces partial observability, individual $Q_a$ networks are commonly RNNs made of GRU \citep{Chung2014EmpiricalModeling}.
A typical architecture of such a network is presented in Figure \ref{fig:ch3_indivQ}.
When evaluating CTDE methods, IQL is commonly tested by training the same architecture presented in this Figure, allowing for comparison with a fully decentralised training of such a network.

The optimisation procedure follows the same principles of the DQN algorithm, and the loss applied to $Q_{mix}(s_t, \mathbf{u_t})$ is defined in Equation~\ref{eq:QMIX_loss}.
Actions are selected with an epsilon greedy policy from $Q_a$ during training and with a greedy policy for testing.
Since this method trains recurrent neural networks, the replay buffer stores sequences of contiguous transitions instead of isolated transitions $\langle s_{t},\mathbf{u_{t}},r_{t},s_{t+1}\rangle$.
Individual $Q_a$ networks and the mixer are all copied to produce target networks represented by $\theta'$.

\begin{equation}
    \mathcal{L}(\theta) = \mathbb{E}_{\langle s_{t},\mathbf{u_{t}},r_{t},s_{t+1} \rangle \sim B}
    \bigg[  
    \big(r_{t} + \gamma \max_{\mathbf{u} \in \mathcal{U}} Q_{tot}^{mix}(s_{t+1}, \mathbf{u}; \theta')
    - Q_{tot}^{mix}(s_{t}, \mathbf{u_{t}}; \theta)\big)^{2}
    \bigg] 
    \label{eq:QMIX_loss}
\end{equation}

\subsection{MAVEN}
\citet{Mahajan2019MAVEN:Exploration} defined the class of state-joint-action value functions that QMIX cannot represent due to poor exploration because of its monotonicity constraint.
They demonstrated the existence of payoff matrices in an n-player game with more than three actions per agent for which QMIX learns a suboptimal policy for any training duration and for epsilon greedy and uniform exploration.
To tackle this problem, they modified the individual $Q_a$ architecture by adding a latent space that influences agent behaviour.
Agents thus learn an ensemble of policies to improve their exploration capabilities.

Specifically, the latent variable is the input of a second and new hypernetwork that computes parameters of the fully connected layer linking recurrent cells to the $Q$-values, outputs of the individual $Q_a$ networks.
This latent variable $z$ is generated per episode by a hierarchical policy network, taking as input the initial state of the environment and a random variable (typically discrete and sampled from a uniform distribution).
The underlying idea is that the latent variable corresponds to different learnt strategies, and the goal of the hierarchical policy network is to select the best strategy based on the initial state $s_0$, which is considered known at testing.
The architecture of the individual $Q_a$ network of MAVEN is represented in Figure~\ref{fig:maven}.

\begin{figure}
\centering
\input{tex_thesis/tikz/ch3/maven}
\caption{MAVEN modification of the individual $Q_a$ network.}
\label{fig:maven}
\caption{Details of the MAVEN architecture.}
\end{figure}

MAVEN's network objective function comprises three parts.
Some parameters must be fixed when computing some parts, meaning they will not be updated based on them.
The first part of the objective is the loss of QMIX defined in Equation~\ref{eq:QMIX_loss}, and it optimises both hypernetworks and individual networks.
This loss is computed by fixing the hierarchical policy network and so the latent variable $z$.

To optimise the hierarchical policy network, any policy optimisation, such as policy gradient (see Section \ref{sec:ch2_policy_based_methods}), computed with the total sum of rewards per episode can be used.
This second objective is computed by fixing both hypernetworks and individual networks.

To ensure that different values of $z$ imply different behaviours, a mutual information loss between the latent variable and consecutive transitions composes the third part of the objective.
This third part of the objective requires the introduction of a variational distribution.
For further details on the MAVEN optimisation procedure and especially on constructing the mutual information objective, we refer the reader to \cite{Mahajan2019MAVEN:Exploration}.

\subsection{QPLEX}
QPLEX \citep{wang2021qplex} extends QMIX with the dueling structure $Q(s_t, u_t) = V(s_t) + A(s_t, u_t)$ \citep{wang2016dueling}, learning a factorisation of $V$ and $A$ with transformers \citep{vaswani2017attention}.
The advantage function $A$ has been introduced in Section \ref{sec:ch2_policy_based_methods}.
The duelling structure involves learning $V(s_t)$ and $A(s_t, u_t)$, typically with a single network with a common part used to form two different heads.
By separating the computation of the value of a given state and the contributions of different actions in that given state, \citet{wang2016dueling} demonstrated better results than DQN.

Back to QPLEX, \citet{wang2021qplex} demonstrated that if the advantage function $A$ respects the advantage-IGM, then the state action value function $Q = V + A$ respect IGM and the state value function $V$ is not constrained.
The individual $Q_a$ satisfy the advantage-IGM if $Q_{tot}=V_{tot}+A_{tot}$ and $V_{tot}(s)=\max_{\mathbf{u}} Q_{tot}(s, \mathbf{u})$ and $Q_a=V_a+A_a$ and $V_a(s)=\max_{u} Q_a(s, u)$ such that Equation \ref{eq:ch3_adv_igm} holds.
This defines the duplex dueling structure of QPLEX.
The constraint on the $Q_a$ is transferred to the advantage functions $A_i$, leading to an unconstrained approximation of the state value function $V$.

\begin{equation}
    \argmax_{\mathbf{u_t}} A_{tot}(s_t, \mathbf{u_t}) =(\argmax_{u_t^{1}} A_{1}(\tau_t^{1}, u_t^{1}),...,\argmax_{u_t^{n}} A_{n}(\tau_t^{n}, u_t^{n}))    \label{eq:ch3_adv_igm}
\end{equation}



\section{Policy-based methods}
\label{sec:ch3_policy}
Policy-based methods learn directly the optimal policy through a neural network $\pi(s, u;\theta)$ that maximises $J(\theta)=\mathbb{E}_{\pi_\theta}[R_0]$.
The well-known REINFORCE method \citep{williams1992simple} ascends the gradient $\nabla_\theta J = \mathbb{E}[\sum_t R_t \nabla_\theta \log \pi(u_t|s_t;\theta)]$ to find $\pi^*$.
Actor-critic methods \citep{sutton1999policy,konda1999actor} expand upon this method by incorporating a parameterised critic that estimates $Q(s_t, u_t;\phi)$, replacing $R_t$, with the actor serving as the parameterised policy.
To reduce variance, a baseline $b(s)$ is injected into the gradient, usually $b(s) = V(s)$, and $Q(s, u;\phi)$ is replaced by $A(s,u; \phi)$ \citep{10.5555/2074022.2074088}, leading to the new gradient expression $\nabla_\theta J = \mathbb{E}[\sum_t A(s_t, u_t; \phi) \nabla_\theta \log \pi(s_t, u_t; \theta)]$.
Advantage estimation is accomplished either by $A(s_t,u_t; \phi)=Q(s_t, u_t;\phi)-\sum_u \pi(u|s_t;\theta) Q(s_t,u; \phi)$ or by $A(s_t,u_t; \phi)=r_t +\gamma V(s_{t+1};\phi) - V(s_t;\phi)$.
Extending these methods to MARL is a straightforward process and the decentralised solution is named IAC \citep{Foerster2017}. 
This approach involves each agent learning independently an actor and a critic, based only on the tuple $(\tau, o)$.
However, this solution does not exploit the additional information provided by the state $s$.
During training, the critic may exploit the full state $s$, which would result in a centralised critic.
However, this approach provides the same feedback to all agents, missing out on the crucial aspect of credit assignment \citep{chang2003all}. 
To address this, MADDPG \citep{lowe2017multi} allows each agent to learn its own critic $Q_a(s, \boldsymbol{u};\phi)$ that is considered centralised since its use of $s$ and $\boldsymbol{u}$.




Naive learner: Independent Actor-Critic (IAC)
Each agent learns its actor and critic independently.

Two possible critics:
IAC-V:  $A(\tau_t, u_t; \phi) = r + \gamma V(\tau_{t+1}; \phi) - V(\tau_t; \phi)$.
IAC-Q:  $A(\tau_t, u_t; \phi) = Q(\tau_t, u_t; \phi) - \sum_{u^a}\pi_\theta(\tau_t, u^a)Q(\tau_t, u^a; \phi)$.
Problem: How to benefit from centralised information such as $s_t$?




\subsection{COMA}

Solutions proposed by Foerster, et. al. (2018) (COMA):
Centralised used during training critic that computes the advantage based on $s_t$.

$$
A(s_t,u^a_t; \phi) = r_t + \gamma V(s_{t+1}; \phi) - V(s_t; \phi)
$$


Problems: 
Based on global rewards $r_t$.
The centralised critic does not solve the credit assignment problem.  

Solution: use a counterfactual baseline.
Inspired from difference reward: 

$$D^a=r(s, \mathbf{u}) - r(s, (\mathbf{u}^{-a}, c^a))$$

The common reward $r(s, \mathbf{u})$ is compared to a reward obtained when agent $a$ executes a default action $c^a$, actions of other agents ($\mathbf{u}^{-a}$) unchanged.
Any action $u^a$ that maximises $r(s, \mathbf{u})$ also maximises $D^a$.

Limitations:
A simulator is required to obtain $r(s, (\mathbf{u}^{-a}, c^a))$.
But these can be approximated.
Decide which action is $c^a$.

Use a centralised critic that computes difference rewards by learning $Q(s, \mathbf{u})$.

For each agent $a$, the advantage is:
$$ A^a(s, \mathbf{u}) = Q(s, \mathbf{u}) - \sum_{u'^{a}} \pi^a(u'^{a}|\tau^a) Q(s,(\mathbf{u^{-a}},u'^{a}))  $$

Problem: $(|\mathcal{U}_1|\*...\*|\mathcal{U}_n|)$ $Q$ values must be computed.

In practice, a $Q$ network has one ouput for each possible action.
Here, this leads to $|\mathcal{U}_1|\*...\*|\mathcal{U}_n|$ outputs which is impractical.
Solution: the critic takes as input $u^{-a}$ and computes only $|\mathcal{U}_a|$ outputs.

This method is only possible for discrete action spaces! 
It is possible to evaluate $\sum_{u'^{a}} \pi^a(u'^{a}|\tau^a) Q(s,(\mathbf{u^{-a}},u'^{a}))$
Monte Carlo 
Gaussian policies in continuous action spaces.



On the other hand, COMA \citep{Foerster2017} and FACMAC \citep{peng2021facmac} propose solutions with a single centralised critic.
COMA, inspired by difference reward \citep{wolpert2001optimal}, proposes having the centralised critic compute a counterfactual baseline for each agent.
For an agent $a$, the difference reward is $R(s_{t+1}, s_t, \boldsymbol {u_t}) - R(s_{t+1}, s_t, (\boldsymbol{u_t}^{-a}, c_t^a))$ where $c$ is a default action.
Computing this requires simulating the environment steps several times.
But in COMA, the centralised critic computes the advantage $A_a(s_t,\boldsymbol{u_t}; \phi)=Q(s_t, \boldsymbol{u};\phi) - \sum_{u'^{a}} \pi({u'^{a}} |\tau_t^a;\theta) Q(s_t, (\boldsymbol{u_t}^{-a}, u'^{a}); \phi)$ for each agent, allowing it to approximate $A$ without more environment steps.
This has the cost of requiring to increase the input space of $A$ as it additionally takes $u^{-a}$ actions as input.

\subsection{FACMAC}

FACMAC suggests using a central but factored critic by employing the value function factorization of QMIX.
The joint-action $Q(s, \boldsymbol{u};\phi)$ is built as a function of $Q_a(\tau, u^a)$, without the need to satisfy IGM and without the constraints on the hypernetwork.


\section{Related work}
In addition to QMIX \citep{Rashid2018}, QVMix \citep{leroy2020qvmix} and QPLEX \citep{wang2021qplex}, QTRAN \citep{Son2019QTRAN:Learning} and Weighted-QMIX \citep{rashid2020weighted} factorise $Q_{tot}$ differently from QMIX, but do not always satisfy IGM \citep{Son2019QTRAN:Learning}.
Other methods, such as LAN \citep{avalos2023local}, also extend over QMIX and learns to cooperate without factorising $Q_{tot}$.

MADDPG \citep{lowe2017multi} is another well-established method, which does not learn a single centralised critic, but one per agent and is designed for continuous action spaces.
Another method, LIIR \citep{Du2019LIIRLearning}, aims to provide credit assessment with individual intrinsic rewards, while HATPRO and HAPPO \citep{kuba2021trust} demonstrate that popular actor-critic methods like TRPO \citep{schulman2015trust} and PPO \citep{schulman2017ppo} can be extended to cooperative MARL tasks.
HATRPO and HAPPO could have been a great addition to our study but are unfortunately not implemented within the PyMarl library.

Another approach for dealing with cooperative multi-agent settings is to link the recent success of sequence models and reinforcement learning by using a multi-agent transformer (MAT) that learns to transform a sequence of observations into a sequence of actions, one per agent \citep{wen2022multiagent}.

critic usefuell paper?

Mean field game

\todo{check state-action or state action}

communication

