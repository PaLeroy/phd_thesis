\chapter{Cooperation}\label{ch:cooperation}
\begin{chapter_outline}

This chapter introduces the main concepts of learning to cooperate with multi-agent reinforcement learning.
While chapters~\ref{ch:qvmix} and~\ref{ch:impmarl} present each a single contribution in this field, this chapter covers the related works in the literature and the basics.
After a small introduction in Section~\ref{sec:ch3_intro}, we define the decentralised POMDP, an adaptation of the partially observable stochastic game to the cooperative setting in Section~\ref{sec:ch3_decpomdp}.
Once defined, we present examples of Dec-POMDP in Section~\ref{sec:ch3_env}, followed by a more precise description of the StarCraft Multi-agent challenge, an environment suite of importance in this manuscript.
We then present methods designed to solve such environments, respectively value-based methods in Section~\ref{sec:ch3_value} and policy-based methods in Section~\ref{sec:ch3_policy}.
For both families, we detail methods used in the next chapters of this manuscript and list others from the literature.

\end{chapter_outline}

\damien{13/02}
\section{Introduction}
\label{sec:ch3_intro}
As introduced in Part~\ref{part:background}, cooperation is the multi-agent setting of agents sharing a common goal.
This manuscript considers the decentralised POMDP (Dec-POMDP) \citep{DecPomdp}, a framework where all agents receive the same reward.
Its definition, provided in Section~\ref{sec:ch3_decpomdp}, can be easily derived from the partially observable stochastic game one (see Section~\ref{sec:ch2_partial_observability}) by having a single reward.
Such a framework is also called common reward games, e.g. in \cite{marl-book}.

Nevertheless, when it comes to cooperation in a multi-agent system, one notion to discuss is whether the action selection and the training of agents are centralised or decentralised.
The difference lies in the information shared or not by agents.
This is referred to, in \citep{marl-book}, as the modes of execution and training.
This manuscript considers three combinations, hereafter described: centralised training and execution, decentralised training and execution, and centralised training with decentralised execution.

Since all agents share a common goal, we could centrally select all actions, sharing common knowledge.
This is the centralised mode, where agents are commonly trained with single-agent reinforcement learning methods.
For example, controlling a robotic hand composed of several actuators can be done with a single agent controlling every actuator.
However, as presented in Section~\ref{sec:ch2_partial_observability}, all agents may not access the same information or may perceive only a part of the state of the environment.
It would then be impossible, except in a simulated environment, to consider that agents can easily select actions in a centralised mode, with information not available to every one of them.

On the contrary, the decentralised execution mode is the one that considers that each agent has only access to a subset of the information, the one it perceives, and uses it to select its action.
The corresponding decentralised training mode is when these agents are trained based only on this perceived information.
We thus refer to the decentralised mode when training agents independently with single-agent reinforcement learning.
In other words, we assume they are the single agent learning in the environment.

Finally, the third mode unifies the two former and is the centralised training with decentralised execution (CTDE).
It allows decentralised execution, with each agent selecting actions based on their observations, but allows for the exploitation of more information during training.
Indeed, RL agents are usually trained in a simulator, having access to every piece of information.
Therefore, using the state of the environment or the actions made by other agents is commonly possible during training.

Back to the research question highlighted in Chapter \ref{ch:introduction}, one can already understand that there are several ways to consider, or not, the other learning agents when training one.
This Part~\ref{part:coop} focuses on methods that exploit the CTDE mode to solve cooperative tasks.
This chapter presents Dec-POMDP environments and CTDE methods issued from the literature. 
In contrast, the following chapters present our contributions: another CTDE method in Chapter~\ref{ch:qvmix} and a real-world application of Dec-POMDP in Chapter~\ref{ch:impmarl}.


\section{Decentralised partially observable Markov decision process}
\label{sec:ch3_decpomdp}
As introduced, the definition of the decentralised partially observable Markov decision process (Dec-POMDP) \citep{DecPomdp} can be derived from the partially observable stochastic game definition.
It is substantially the same, but the reward function maps only to a single real, a reward common to all agents.

We define the Dec-POMDP by a tuple $[\mathcal{S}, \mathcal{Z}, \mathcal{U}, n, O, R, P, \gamma]$, where $n$ agents $a_i$, $i \in \mathcal{A} \equiv \{1,..,n\}$, simultaneously choose an action at every time step $t$.
The state of the environment is $s_t \in \mathcal{S}$ where $\mathcal{S}$ is the set of states.
The observation function $O: \mathcal{S} \times \mathcal{A} \rightarrow \mathcal{Z}$ maps the state to an observation $o_t^{a} \in \mathcal{Z}$ perceived by agent $a$ at time $t$, where $\mathcal{Z}$ is the observation space.
Each agent $a_i$ selects an action $u_t^{a_i} \in \mathcal{U}^{a_i}$, and the joint action space is $\mathcal{U}$.
After the joint action $\boldsymbol{u_t} \in \mathcal{U}$ is executed, the transition function determines the new state with probability $P(s_{t+1}|s_t, \boldsymbol{u_t}): \mathcal{S} \times \mathcal{S} \times\mathcal{U} \rightarrow  [0,1] $, and $r_t=R(s_{t+1}, s_t, \boldsymbol {u_t}): \mathcal{S} \times \mathcal{S} \times \mathcal{U} \rightarrow \mathbb{R}$ is the team reward obtained by all agents.

An agent's policy is a function $\pi^{a}(u_t^{a}|\tau_t^{a},o_t^{a}): (\mathcal{Z} \times \mathcal{U}^a)^t \rightarrow [0,1]$, which maps its history $\tau_t^{a} \in (\mathcal{Z} \times \mathcal{U}_a)^{t-1}$ and its observation $o_t^{a}$ to the probability of taking action $u_t^{a}$. 
The joint or team policy is denoted by $\boldsymbol{\pi}=(\pi^1,..,\pi^n)$.
The cumulative discounted reward obtained from time step $t$ over the next $T$ time steps is defined by $G_{t} = \sum_{k=0}^{T-1} \gamma^k r_{t+k}$ and $\gamma \in [0, 1)$ is the discount factor.
The goal of agents is to find the optimal joint policy that maximises the expected $G_t$ during the entire episode: $\boldsymbol{\pi^{*}} =\argmax_{\boldsymbol{\pi}} \mathbb{E}[G_0|\boldsymbol{\pi}]$.
Again, we emphasise that this expected sum depends on the joint policy.

\todo{From the introduction, the mode of executions and training, based on the special case of oliehook, MMDP, MPOMDP, observability, central model, factored model}
\todo{Challenges in Dec-POMDP,  + solved for what already? cf decpomdp book}

This is called a jointly observable Dec-POMDP that is also referred to as a decentralised MDP in \citep{DecPomdp}.
However, this does not make the problem simpler because each agent still does not observe this state \citep{bernstein2002complexity}.
\todo{check if we do not want to address this in chapter 3.}

\todo{Convergence ? Solutions? Nash etc?}

\section{Environments}
\label{sec:ch3_env}
Before diving into the details of CTDE methods, we provide an overview of the existing suites\footnotemark  of environments from the literature.
We then describe the StarCraft multi-agent challenge (SMAC), perhaps one of the most studied environment suites in the community, but also used in our experiments in Chapter~\ref{ch:qvmix}.
Moreover, Chapter~\ref{ch:impmarl} is dedicated to a specific suite of environments, and we leave its description there.

\footnotetext{In this manuscript, an environment refers to an instance of a particular framework (here, of a Dec-POMDP). In the literature, we commonly see it to refer to either a collection of environments or an instance of one. We use "suite" to distinguish between these two.}

The multi-agent particle environments (MPE) \citep{lowe2017multi} is a popular suite in the MARL community with cooperative scenarios.
In MPE, particles move in a 2D grid with continuous action and state spaces.
This suite comes with all types of multi-agent settings: competition, cooperation, with and without competition.
A second environment with continual action space is MaMuJoCo \citep{peng2021facmac}.
MuJoCo \citep{todorov2012mujoco} stands for multi-joint dynamics with contact.
It is a famous physics-based simulator to learn to control, for example, a humanoid to run.
MaMuJoCo essentially factorises MuJoCo's decisions by decentralising the decision, typically having a different agent for each leg.
It is an example of a SARL environment extended to MARL.
Other cooperative environments based on game simulators include the Hanabi Challenge \citep{Bard_2020}, a "cooperative solitaire" between two and five players, and Google Research Football \citep{kurach2020google}, a football game simulator.

Cooperative MARL methods are mostly benchmarked on these games and simulators, but real-world applications also exist.
City flow \citep{zhang2019cityflow} are traffic control environments where agents control traffic lights in a city.
In Flatland \citep{mohanty2020flatland}, agents control trains to solve a scheduling problem, avoiding collisions by coordinating not to take the same routes.
The multi-robot warehouse \citep{papoudakis2021benchmarking, christianos2020shared} simulates warehouse agents needing to deliver requested goods.
Multi-Agent Tracking Environment (MATE) \citep{NEURIPS2022_b2a1c152} is a target coverage control problem where cameras are controlled to detect all targets.
A mixed cooperative-competitive game can be made by controlling the targets.
Many more examples of cooperative MARL applications exist.
Many can be found in the book of \cite{marl-book}, in the review on cooperative MARL done by \cite{oroojlooy2022review}, such as autonomous driving, resource allocation, stock market,...
\todo{mosaic avec les environnements}

\subsection{StarCraft multi-agent challenge} \label{sec:ch3_smac}
The StarCraft multi-agent challenge (SMAC) \citep{samvelyan2019starcraft} and its improved version SMACv2 \citep{ellis2023smacv2} are probably the most studied suite of environments with CTDE methods.
SMAC is based on the StarCraft II Learning Environment \citep{vinyals2017starcraft}, an RL environment to play StarCraft II (SC2).
StarCraft is a strategy video game where players compete by managing units, gathering resources, building an army and defeating opponents.
In such games, micro-management refers to unit management, unlike macro-management, which involves resource management.
Unlike playing the real game of StraCraft like in AlphaStar \citep{vinyals2019grandmaster}, SMAC is a suite of micro-management challenges where each game unit is an agent.
Many scenarios exist in SMAC, also called "maps", and all involve training a team to achieve a common goal.
Still, this common goal is to defeat an adversarial team controlled by the game's deterministic and stationary policy built-in AI.
In Figure~\ref{fig:ch3_smac}, we present the initial configurations of two maps where two teams face each other.
Also note that in Chapter~\ref{ch:2teams}, we present a contribution where SMAC has been modified to train both competing teams.

\begin{figure}
     \centering
     \begin{subfigure}[b]{0.5\textwidth}
         \centering
         \includegraphics[height=2.3cm]{tex_thesis/figures/ch3/3m_screen.pdf}
         \caption{3m}
         \label{fig:ch3_3m}
     \end{subfigure}%
     \begin{subfigure}[b]{0.5\textwidth}
         \centering
         \includegraphics[height=2.3cm]{tex_thesis/figures/ch3/3s5z_screen.pdf}
         \caption{3s5z}
         \label{fig:ch3_3s5z}
     \end{subfigure}
    \caption{Two scenarios of the StarCraft multi-agent challenges.}
    \label{fig:ch3_smac}
\end{figure}

We hereafter toughly define the elements of the Dec-POMDP implemented in SMAC.
Agents have partial observability defined by a sight range, a circle inside which they can observe other agents.
There are different components in the observation.
An agent observes information about itself: its remaining hit points and shield points and four booleans representing the direction it can move in (NSWE). 
It also observes information about other agents within its sight range: the relative distance, relative x, relative y and the remaining hit points and shield points. 
If the other agent is an ally, it also observes its last performed action performed.
When the other agent is an enemy, it observes if this agent is within shooting range.
The shooting range is smaller than the sight range and depends on the unit type, and some units even attack in melee.

The state of the environment accumulates all information from agents' observations.
As described, an agent perceives other agents with distances relative to itself and does not know where it is on the map.
In the state, the agents' positions are encoded with their coordinates relative to the centre of the map.'

Within all scenarios, the agent has the choice of eight actions: do nothing, move in one of four directions (NSWE) or attack one of its three opponents.
Some actions are forbidden in SMAC, such as an attack if the opponent is not within shooting range.
Therefore, agents must consider which ones are available before choosing an action.

At each timestep, the agent receives a zero or positive reward, common to each agent of the same team. 
This reward is a sum of three factors: a zero or positive reward for the damage dealt, a positive reward if an enemy unit's hit points reach zero, and a positive reward if all enemy units are defeated. 
Maximising the reward forces the team to neutralise every unit of the opposing team.

We hereafter describe both scenarios of Figure~\ref{fig:ch3_smac}.
There are other types of units in other scenarios, and we refer the reader to \citep{samvelyan2019starcraft}.
In the $3m$ scenario, represented in Figure~\ref{fig:ch3_3m}, six marines compete in two teams of three.
A marine has $45$ hit points and shoots at range, inflicting $6$ damage points to an opponent for each attack.
In the $3s5z$ scenario, represented in Figure~\ref{fig:ch3_3s5z}, six stalkers and ten zealots compete in two teams of eight.
Both units have shield points in addition to hit points.
A shield receives a different amount of damage and regenerates over time if the unit is not attacked again for a given time.
A stalker has $80$ hit points and $80$ shield points.
It shoots at range, inflicting $13$ damage points to the shield, $12$ damage points to a zealot's hit points and $17$ to a stalker's hit points.
A zealot has 100 hits points and 50 shield points.
It attacks in melee and inflicts $16$ damage points to the shield and $14$ damage points to the hit points of a zealot or a stalker.
All maps are presented in a video from the author\footnote{\url{https://youtu.be/VZ7zmQ_obZ0}}.

Finally, it can be intriguing to see an environment where two teams compete is considered cooperative.
This is because the built-in AI is stationary. 
These agents are not learning agents and can be considered part of the environment.
The built-in AI strategy is a rules-based policy.
Specifically, each agent moves toward the starting point of the opponent's team until it reaches the opposite side of the map and stops.
If they encounter opponents in their sight range, they select one as their target based on a priority score.
They will choose to attack the closest unit with the highest priority, which will remain the target until its priority drops or until it can no longer be attacked.
A unit's priority score is based on its type and current action.
For example, if two of the same units attack and the targeted unit stops attacking, its priority score will drop, and the built-in AI agents will select the other unit to attack.
One of the weaknesses of this built-in AI strategy that the learning agents need to learn is to stop attacking to stay alive, with the condition that an ally should be attacking.
So, agents need to cooperate to attack or not share the possible damage inflicted by the opponents.
The next sections cover the MARL methods to achieve this coordination.

\section{Value-based methods}
\label{sec:ch3_value}

As detailed in Chapter \ref{ch:background}, value-based methods aim to learn the optimal state-action value function $Q^{\pi^*}(s, u)$, such that the optimal policy is $\pi^*(s)=\argmax_{u} Q^{\pi^*}(s, u)$.
In SARL, one solution, called DQN, is to approximate $Q$ with a neural network $\theta$ and learn $Q(s, u;\theta)$
by minimising the loss defined in Equation~\ref{eq:ch2_dqnloss}.

As introduced in Section~\ref{sec:ch3_intro}, there exist different modes of training and execution.
In the first mode, one possible centralised training and execution approach is DQN to train a centralised learner in a Dec-POMDP.
However, it is only possible if all agents can access the state $s$ during execution.
Such a setting is also called a multi-agent Markov decision process \citep{boutilier1996planning}.
In this setting, the centralised agent learns the state-joint-action value function $Q^{\boldsymbol{\pi}}(s,\mathbf{u}; \theta)$ by following the adapted DQN loss
\begin{equation}
\label{eq:ch3_centralQloss}
    \mathcal{L}(\theta) = \mathbb{E}_{\langle . \rangle\sim B} \big[\big(r_{t} + \gamma \max_{\mathbf{u}} Q(s_{t+1}, \mathbf{u}; \theta')- Q(s_{t}, \mathbf{\mathbf{u_t}}; \theta)\big)^{2}\big].
\end{equation}
Issues are that the joint action space scales exponentially with $n$.
Also, in practice, agents select their action based only on their history $(o, \tau)$ and not the state $s$.

For the second mode, the decentralised training and execution, each agent can learn its own Q-value independently $Q_a=Q(\tau^a, u^a)$, agnostically of the existence of other agents.
IQL \citep{Tan1993} is the extension of Q-Learning (see Chapter~\ref{ch:background}) to this mode. 
The equivalent extension has been performed with DQN \citep{TampuuDqnIQL}, also called IQL.
Typically, in a Dec-POMDP, a recurrent network approximates the independent Q-value, as explained in Section \ref{sec:ch2_partial_observability}.
One problem with IQL is that agents must select actions which maximise $Q(s_t, \mathbf{u_t})$ while ignoring, at any time, actions taken by other agents.

This is where centralised training with decentralised execution (CTDE), the third mode, comes in handy.
With CTDE methods, it is possible to approximate $Q(s_t, \mathbf{u_t})$ as a factorisation of individual $Q_a$ functions during training such that $\mathbf{u_t}$ maximises both the joint and the individual $Q_a$ functions.
To ensure this, individual $Q_a$ functions must satisfy the individual-global-max condition (IGM) \citep{Son2019QTRAN:Learning}
\begin{equation}
    \argmax_{\mathbf{u_t}} Q(s_t, \mathbf{u_t}) =(\argmax_{u_t^{1}} Q_{1}(\tau_t^{1}, u_t^{1}),...,\argmax_{u_t^{n}} Q_{n}(\tau_t^{n}, u_t^{n})).
    \label{eq:ch3_igm}
\end{equation}

Satisfying the IGM property allows agents to greedily select actions that maximise their individual $Q_a$ and the state-joint-action value function $Q(s_t, \mathbf{u_t})$.
Therefore, during training, an approximation of $Q(s_t, \mathbf{u_t})$, sometimes denoted $Q_{tot}$, is built as a factorisation of $Q_a$ and then dropped at execution.
Agents select actions based on these $Q_a$, satisfying IGM, and we keep only these networks at execution.
Note that these $Q_a$ are now utility functions because they do not approximate the expected sum of discounted rewards.
In this manuscript, we express $Q_{tot}$ as a function of the state $s$ and consider that the state is known.
The following expressions can be rewritten as a function of the joint history $\mathbf{\tau}$ if the state $s$ is unknown during training.

Maybe one of the first methods of value-based factorisation is called value decomposition network (VDN) \citep{sunehag2018vdn}.
This method factories $Q_{tot}$ using the addition, an operation which satisfies IGM: $Q_{tot}^{VDN}(s_t, \mathbf{u_t}) = \sum_{i=1}^n Q_{a_i}(\tau^{a_i}_t, u^{a_i}_t)$.
Leaving the details of the training procedure for later, one limit of VDN is that factorising through addition does not allow to approximate complex joint-action-state Q functions.
Another limitation is that we need $Q_{tot}$ only during training, and in VDN, we do not benefit from additional information at our disposal, such as the state.
In the following, we present three other methods that have been used in the experiment conducted in the presented contributions.
Others exist, and we finish this section with a related work discussion.

\subsection{QMIX}
QMIX \citep{Rashid2018} is a CTDE method where the approximation of $Q_{tot}$ is performed by a monotone factorisation of the individual $Q_a$ functions, also function of the state during training:
\begin{equation}
     Q_{tot}^{mix}(s_t, \mathbf{u_t})=\text{Mixer} \left(Q_{a_1}(\tau^{a_1}_t, u_t^{a_1}) ,..,Q_{a_n}(\tau^{a_n}_t, u_t^{a_n}), s_t\right).
     \label{eq:ch3_qmixappendix}
\end{equation}

The monotonicity is ensured by a hypernetwork \citep{Ha2016HyperNetworks} $h_p(.): \mathcal{S} \rightarrow \mathbb{R}^{|\phi|+}$ which computes, from the state $s_t$, the parameters $\phi$ of a mixer network $h_m(.;\phi):\mathbb{R}^n \times \phi \rightarrow \mathbb{R}$.
To ensure monotonicity, the weights (and not the biases) defined by $\phi$ are constrained to be positive.
Together, $h_p$ and $h_m$ defines the mixer such that $ Q_{tot}^{mix}(s_t, \mathbf{u_t}) = h_m\left(Q_{a_1},..,Q_{a_n}; h_p(s_t)\right)$.

\begin{figure}
\centering
\input{tex_thesis/tikz/ch3/qmix}
\caption{QMIX architecture. Individual $Q_a$ are in red, and the mixing network is in blue.}
\label{fig:qmix}
\end{figure}

The monotonicity of $Q_{tot}^{mix}$ with respect to the individual $Q_a$ functions, 
\begin{equation}
    \frac{\partial Q_{tot}^{mix}(s_t, \mathbf{u_t})}{\partial Q_{a}(s_t, u_t^{a})} \geq 0 \text{ } \forall a
    \label{eq:ch3_monotonicity},
\end{equation}
is satisfied because a neural network comprised of monotonic functions ($h_m$) and strictly positive weights ($h_p$) is monotonic with respect to its inputs ($Q_a$). 
The entire QMIX architecture is presented in Figure~\ref{fig:qmix}.

The optimisation procedure follows the same principles of the DQN algorithm, and the loss applied to $Q_{mix}(s_t, \mathbf{u_t})$ is 
\begin{equation}
    \mathcal{L}(\theta) = \mathbb{E}_{\langle s_{t},\mathbf{u_{t}},r_{t},s_{t+1} \rangle \sim B}
    \bigg[  
    \big(r_{t} + \gamma \max_{\mathbf{u} \in \mathcal{U}} Q_{tot}^{mix}(s_{t+1}, \mathbf{u}; \theta')
    - Q_{tot}^{mix}(s_{t}, \mathbf{u_{t}}; \theta)\big)^{2}
    \bigg].
    \label{eq:QMIX_loss}
\end{equation}
During training, actions are selected with an epsilon greedy policy from $Q_a$ and at testing, with a greedy policy.
Individual $Q_a$ networks and the mixer are all copied to produce target networks represented by $\theta'$.
\begin{figure}
    \centering
\input{tex_thesis/tikz/ch3/indivQ}
\caption{Common $Q_a$ network implementation. The hidden state $h$ embeds the history, and the action space size defines the number of outputs of the network.}
\label{fig:ch3_indivQ}
\end{figure}

Since the Dec-POMDP induces partial observability, individual $Q_a$ networks are commonly RNNs made of GRU \citep{Chung2014EmpiricalModeling} and the replay buffer stores sequences of contiguous transitions instead of isolated transitions $\langle s_{t},\mathbf{u_{t}},r_{t},s_{t+1}\rangle$.
A typical architecture of such a network is presented in Figure \ref{fig:ch3_indivQ}.
When evaluating CTDE methods, IQL is commonly tested by training the same architecture as the individual network, allowing for comparison with a fully decentralised training of such a network.

\subsection{MAVEN}
\citet{Mahajan2019MAVEN:Exploration} defined the class of state-joint-action value functions that QMIX cannot represent due to poor exploration induced by the monotonicity constraint.
They demonstrated the existence of payoff matrices in an n-player game with more than three actions per agent for which QMIX learns a suboptimal policy for any training duration with both epsilon greedy and uniform exploration.
To tackle this problem, they modified the individual $Q_a$ architecture by adding a latent space that influences agent behaviour.
Agents thus learn an ensemble of policies to improve their exploration capabilities.

Specifically, the latent variable is the input of a second and new hypernetwork that computes parameters of the fully connected layer linking recurrent cells to the $Q$-values, outputs of the individual $Q_a$ networks.
This latent variable $z$ is generated per episode by a hierarchical policy network, taking as input the initial state of the environment and a random variable (typically a discrete uniform).
The underlying idea is that the latent variable corresponds to different learnt strategies, and the goal of the hierarchical policy network is to select the best strategy based on the initial state $s_0$, which is considered known at testing.
The new architecture of the individual $Q_a$ network of MAVEN is represented in Figure~\ref{fig:maven}.

\begin{figure}
\centering
\input{tex_thesis/tikz/ch3/maven}
\caption{MAVEN modification of the individual $Q_a$ network.\todo{pas droit}}
\label{fig:maven}
\end{figure}

MAVEN's network objective function comprises three parts.
Some parameters must be fixed when computing some parts, meaning all parameters are not be updated based on the three objectives.
The first part of the objective is the loss of QMIX defined in Equation~\ref{eq:QMIX_loss}, and it optimises both hypernetworks and individual networks.
This loss is computed by fixing the hierarchical policy network and so the latent variable $z$.

To optimise the hierarchical policy network, any policy optimisation, such as policy gradient (see Section \ref{sec:ch2_policy_based_methods}), optimising the sum of rewards per episode can be used.
This second objective is computed by fixing both hypernetworks and individual networks.

The third part of the objective ensures that different values of $z$ imply different behaviours. 
It is a mutual information loss between the latent variable and consecutive transitions.
It requires the introduction of a variational distribution. 
For further details on the MAVEN optimisation procedure and especially on constructing the mutual information objective, we refer the reader to \cite{Mahajan2019MAVEN:Exploration}.

\subsection{QPLEX}
QPLEX \citep{wang2021qplex} extends QMIX with the dueling structure $Q(s_t, u_t) = V(s_t) + A(s_t, u_t)$ \citep{wang2016dueling}, learning a factorisation of $V$ and $A$ with transformers \citep{vaswani2017attention}.
The advantage function $A$ has been introduced in Section \ref{sec:ch2_policy_based_methods}.
The duelling structure involves learning $V(s_t)$ and $A(s_t, u_t)$, typically with a single neural network with a common backbone to form two different heads.
By separating the computation of the value of a given state and the contributions of different actions in that given state, \citet{wang2016dueling} demonstrated better results than DQN.

Back to QPLEX, \citet{wang2021qplex} demonstrated that if the advantage function $A$ respects the advantage-IGM, then the state action value function $Q = V + A$ respect IGM while removing constraints on the state value function $V$.
The individual $Q_a$ satisfy the advantage-IGM if $Q_{tot}=V_{tot}+A_{tot}$ and $V_{tot}(s)=\max_{\mathbf{u}} Q_{tot}(s, \mathbf{u})$ and $Q_a=V_a+A_a$ and $V_a(s)=\max_{u} Q_a(s, u)$ such that 
\begin{equation}
    \argmax_{\mathbf{u_t}} A_{tot}(s_t, \mathbf{u_t}) =(\argmax_{u_t^{1}} A_{1}(\tau_t^{1}, u_t^{1}),...,\argmax_{u_t^{n}} A_{n}(\tau_t^{n}, u_t^{n}))    \label{eq:ch3_adv_igm}
\end{equation}
holds.
This defines the duplex dueling structure of QPLEX.
The constraint on the $Q_a$ is transferred to the advantage functions $A_i$, leading to an unconstrained approximation of the state value function $V$.

\subsection{Other methods}
In addition to QMIX \citep{Rashid2018}, QVMix \citep{leroy2020qvmix} and QPLEX \citep{wang2021qplex}, QTRAN \citep{Son2019QTRAN:Learning} and Weighted-QMIX \citep{rashid2020weighted} factorise $Q_{tot}$ differently from QMIX, but do not always satisfy IGM \citep{Son2019QTRAN:Learning}.
Other methods, such as LAN \citep{avalos2023local}, also extend over QMIX and learns to cooperate without factorising $Q_{tot}$.

\section{Policy-based methods}
\label{sec:ch3_policy}
\todo{remove o from policy and tau}

Policy-based methods learn the optimal policy directly through a neural network, as introduced in Chapter \ref{ch:background}.
Extending these methods to Dec-POMDP is a straightforward process, and a decentralised solution is named IAC \citep{foerster2017coma}. 
It is the independent method corresponding to IQL for policy-based methods.
The policies learned by the actor of each agent is $\pi^{a}(u_t^{a}|\tau_t^{a}, o^a_t;\theta)$.
The policy parameters are updated by ascending the gradient
\begin{equation}
\label{eq:ch3_policy_grad}
    \nabla_\theta J(\pi^a_\theta) = \mathbb{E}_B\left[A_a(\tau^{a}_t, o_t^a, u_t^{a}; \phi) \nabla_{\theta} \log \pi^{a}(u_t^{a}|\tau_t^{a},  o_t^a;\theta)\right].
\end{equation}
As in SARL, there are two possible ways of estimating the critic, leading to two IAC versions: IAC-V where a state value function is learned $A_a(\tau_t, u_t; \phi) = r + \gamma V_a(\tau_{t+1}; \phi) - V_a(\tau_t; \phi)$ and IAC-Q, where a state-action value function is learned $A_a(\tau_t, u_t; \phi) = Q_a(\tau_t, u_t; \phi) - \sum_{u^a}\pi_\theta(\tau_t, u^a)Q_a(\tau_t, u^a; \phi)$.
The parameters of $\pi^a$ and $Q_a$ are denoted $\theta$ or $\phi$ and not $\theta_a$ or $\phi_a$ to improve readability.

In IAC, each agent independently learns an actor and a critic, but this solution does not benefit from the additional information the state $s$ provides.
Since critics are used only during training, one straightforward solution is to exploit the state $s$ to compute a centralised critic.
We hereafter detail several methods that consider a centralised critic exploiting the state.

\subsection{COMA}
\label{sec:ch3_coma}

COMA stands for counterfactual multi-agent policy gradient and is a solution proposed by \cite{foerster2017coma}.
They start by making the constatation that a centralised critic computing the advantage $A_a(s_t, u^a_t) = r_t + \gamma V(s_{t+1}; \phi) - V(s_t; \phi)$, is based on the common reward $r_t$ but does not consider how the action of an individual agent influences it.
Such a centralised critic does not solve the credit assignment problem.

The proposed solution in COMA is inspired by difference rewards \citep{wolpert2001optimal} and is to use a counterfactual baseline $D^a=R(s_{t+1}, s_t, \boldsymbol {u_t}) - R(s_{t+1}, s_t, (\boldsymbol{u_t}^{-a}, c_t^a))$.
The common reward $R(s_{t+1}, s_t, \boldsymbol {u_t})$ is compared to a reward obtained when agent $a$ executes a default action $c^a$, while we preserve actions of other agents ($\mathbf{u}^{-a}$) unchanged.
Any action $u^a$ that maximises $R(s_{t+1}, s_t, \boldsymbol {u_t})$ also maximises $D^a$.
But there are some immediate limitations.
First, we need to run the simulator to obtain this "default reward" $R(s_{t+1}, s_t, (\boldsymbol{u_t}^{-a}, c_t^a))$.
And we need to do it for all agents.
Second, say we can approximate it, but we still need to decide which action is $c^a$, which would induce additional approximation error.

The solution in COMA is to build a centralised critic that computes difference rewards by learning $Q(s, \mathbf{u})$.
For each agent $a$, the advantage updating agents networks in Equation \ref{eq:ch3_policy_grad} is 
\begin{equation}
\label{eq:ch3_coma_adv}
A_a(s_t,\boldsymbol{u_t}; \phi)=Q(s_t, \boldsymbol{u_t};\phi) - \sum_{u'^{a}} \pi^a({u'^{a}} |\tau_t^a, o_t^a;\theta) Q(s_t, (\boldsymbol{u_t}^{-a}, u'^{a}); \phi).
\end{equation}
This solves the problem of simulating the difference rewards but not the amount of $Q$ values to be computed.
Indeed, $|\mathcal{U}_1|+...+|\mathcal{U}_n|$ values must be computed to obtain the advantage of all agents.
In practice, a $Q$ network has one output for each possible action.
Here, this leads to $|\mathcal{U}_1|+...+|\mathcal{U}_n|$ outputs which would be impractical.
The solution is that the critic takes as input $\boldsymbol{u}^{-a}$ and computes only $|\mathcal{U}_a|$ outputs.
To use the same critic for all agents implies that all action spaces are the same.

However, a major limitation is that this method is only possible for discrete action spaces, especially considering the sum over the possible actions in the advantage.
\cite{foerster2017coma} argue that it would be possible to apply COMA with continuous action spaces easily by evaluating $\sum_{u'^{a}} \pi^a(u'^{a}|\tau^a) Q(s,(\mathbf{u^{-a}},u'^{a}))$ with Monte Carlo estimations or by using policy construction that allows to compute it anatically.
A final observation is that the critic is considered central because using the state $s_t$, but it should be called $n$ times to update the $n$ policy networks.
However, training a single critic by sharing the weights for all agents is possible.

In COMA, policy networks $\theta$ usually follow the same architecture of IQL presented in Figure \ref{fig:ch3_indivQ}, where the outputs are not $Q$ values for each action but the probability of taking each action, usually obtained by applying a softmax to the last layer.
The critic is commonly a typical sequence of fully connected layers that output the $|\mathcal{U}_a|$ from the state $s_t$, the actions of others $\boldsymbol{u_t}^{-a}$ and sometimes additional information, such as $o_t^a$ or the previous taken action $\boldsymbol{u_{t-1}}$.

\subsection{FACMAC}

FACMAC \citep{peng2021facmac} stands for factored multi-agent centralised policy gradients.
They propose to use a centralised but factored critic computing $Q_{tot}$ as a factorisation of individual $Q_a$, like in QMIX.
In COMA, the critic is centralised because it has access to the state, but all agents use it independently to update their decentralised policy.
\cite{peng2021facmac} calls this approach a monolithic critic, and they argue that a non-monolithic factored critic is preferable to scale to many agents or actions.
In addition, they show that estimating gradients independently for each agent can yield sub-optimal joint policies.
This motivates again the centralised and factored critic that can compute a single gradient estimate for the joint policy, improving cooperation capabilities.

Unlike COMA, FACMAC is designed for continuous action spaces and can be adapted to discrete ones.
It is built on the same foundation of another actor-critic method for MARL developed to solve a stochastic game called MADDPG \citep{lowe2017multi} for multi-agent deep deterministic policy gradient.
It is a multi-agent version of DDPG \citep{lillicrap2015continuous}, an actor-critic method using deep neural networks to approximate a deterministic policy and a $Q$ function in SARL.
Since the action space is continuous, obtaining the action $\argmax_u Q(s, u)$ can be difficult.
Instead of computing the arg max, the solution in DDPG is to learn a parametrised deterministic policy $\mu_\theta:\mathcal{S}\rightarrow\mathcal{U}$ that solves $\max_\theta \mathbb{E}_B[Q(s, \mu_\theta(s); \phi)]$ by gradient ascent, while the critic estimates this $Q$ \citep{silver2014deterministic}.
MADDPG uses DDPG to learn decentralised policies using centralised critics with access to the state and actions of other agents.
In MADDPG, each decentralised actor $\mu^a(\tau^a, o^a;\theta)$ is updated by ascending the gradient
\begin{equation}
\label{eq:ch3_maddpg_grad}
    \nabla_\theta J(\mu^a) = \mathbb{E}_B\left[\nabla_{\theta} \mu^a(\tau_t^a, o_t^a;\theta) \nabla_{u^a} Q_a^{\mathbf{\mu}}(s_t, \mathbf{u_t^{-a}}, u_a; \phi)|u^a=\mu^a(\tau_t^a, o_t^a)\right],
\end{equation}
where actions of other agents are taken from the replay buffer while agent a's action is taken from its deterministic policy when computing the state-joint-action value function.
The critic is updated following the same centralised loss adapted from DQN defined in Equation \ref{eq:ch3_centralQloss} where, in addition to a target network to compute $Q$, actions of agents to compute this target value are taken from target deterministic policies.
While MADDPG addresses stochastic games with individual rewards, it can be applied easily in Dec-POMDP, with the drawbacks mentioned earlier.
Moreover, the critic of MADDPG is monolithic and different for each agent, like COMA.

Using a single centralised and factored critic is FACMAC's change to MADDPG.
All agents share this critic that computes $Q_{tot}^{\mathbf{\mu}}$ where $\mathbf{\mu}$ is the joint deterministic policy.
As for the value-based methods, $Q_{tot}$ is a function of individual utility functions $Q_a$.
The factorisation can be done using a sum, such as in VDN, or by using an hypernetwork, such as in QMIX in Equation \ref{eq:ch3_qmixappendix}.
However, for the the monotonicity constraint to satisfy IGM is not required for a critic here.

The new critic is not simply used like in MADDPG by replacing how $Q$ is computed in Equation \ref{eq:ch3_maddpg_grad}.
Indeed, with this gradient, each agent updates its actor network based on the actions of others present in the replay buffer, which would lead to sub-optimal joint-policies.
This leads FACMAC to compute a gradient based on the current joint-policy $\mathbf{\mu}(\mathbf{\tau}) = (\mu^{a_1}(\tau^{a_1}, o^{a_1}), .., \mu^{a_n}(\tau^{a_n}, o^{a_n}))$ given by
\begin{equation}
\label{eq:ch3_facmac_grad}
    \nabla_\theta J(\mu^a) = \mathbb{E}_B\left[\nabla_{\theta} \mathbf{\mu} \nabla_{\mathbf{\mu}} Q_{tot}^{\mathbf{\mu}}(s_t, \mathbf{\tau_t}, \mathbf{\mu}(\mathbf{\tau_t}); \phi)\right].
\end{equation}
Updating the joint-policy instead of independently update them is required and is one of the claim in FACMAC.
Finally, with discrete action space, it is possible to use a Straight-Through GumbelSoftmax \citep{} in order to have a discrete but differentiable policy.

\subsection{Other methods}
MADDPG \citep{lowe2017multi} is another well-established method, which does not learn a single centralised critic, but one per agent and is designed for continuous action spaces.
Another method, LIIR \citep{Du2019LIIRLearning}, aims to provide credit assignment with individual intrinsic rewards, while HATPRO and HAPPO \citep{kuba2021trust} demonstrate that popular actor-critic methods like TRPO \citep{schulman2015trust} and PPO \citep{schulman2017ppo} can be extended to cooperative MARL tasks.

\section{Related work}
The goal of this chapter is not to provide the best CTDE methods, while some comparative results are provided in the next chapters, we also argue that \citep{gorsane2022towards}.

Another approach for dealing with cooperative multi-agent settings is to link the recent success of sequence models and reinforcement learning by using a multi-agent transformer (MAT) that learns to transform a sequence of observations into a sequence of actions, one per agent \citep{wen2022multiagent}.


critic usefuell paper?

Mean field game

communication

Relative overgeneralization (cited in facmac)


\citep{lyu2021contrasting} vs \citep{peng2021facmac}

adversarial agent in the decpompd? how to handle?
