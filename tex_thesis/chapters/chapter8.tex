\chapter{Conclusion}\label{ch:conclusion}

This thesis demonstrates through three contributions the usefulness of considering other learning agents when training one with reinforcement learning in a multi-agent environment.
We provide the necessary background before presenting three specific contributions.
The first contribution extends the DQV single-agent algorithms to cooperative multi-agent ones, outperforming the methods compared against.
The second contribution demonstrates the interest of these cooperative methods in the context of a real-world application: the infrastructure management problem.
The third contribution analyses how to train a team in a mixed cooperative-competitive setting where two teams compete.
We hereafter summarise the three parts of this chapter and discuss the scientific findings before finally discussing the potential societal impact of multi-agent reinforcement learning.

Part \ref{part:background} provides the required background to understand the successive parts.
Specifically, Chapter \ref{ch:background} presents the stochastic game, a general framework for multi-agent reinforcement learning and how it can be adapted when considering specific settings.
Multi-agent reinforcement learning takes its foundation in single-agent reinforcement learning, and we offer a succinct introduction to its basics.
Specifically, we discuss model-based and dynamic programming, focus on value-based and policy-based methods for model-free RL and conclude by addressing the partial observability of agents.

Part \ref{part:coop} tackles the problem of training agents to learn to cooperate.
When agents cooperate, the setting can be framed as a decentralised partially observable Markov decision process where agents share the same reward.
This part comprises three chapters.
One is a background chapter, and the others highlight two specific contributions.

Chapter \ref{ch:cooperation} defines the cooperative framework and applications that can be framed as such a problem.
Several cooperative MARL methods to solve Dec-POMDP are defined following its definition, emphasising how algorithms have been designed to allow centralised training with decentralised execution.
While we postpone comparing the performance of these methods to later chapters, this third chapter concludes with a literature discussion, highlighting works that are not considered in this manuscript but are of interest.

Chapter \ref{ch:qvmix} presents QVMix and other value-based methods for Dec-POMDP.
It highlights that learning the state value function along the state-action value function as it is done in SARL by the DQV family of algorithms can be extended to cooperative CTDE methods in MARL.
Moreover, it allows to outperform the methods compared against and to not overestimate the state-action value function unlike these compared value-based methods relying on the classical Q-learning update.

Chapter \ref{ch:impmarl} concludes Part \ref{part:coop} by presenting infrastructure management planning, a real-world application that can be framed as a Dec-POMDP.
Managing infrastructure by timely inspecting or repairing system components to minimise failure risk and maintenance cost becomes challenging as the system grows in size.
Decentralising the decision, making each component an independent agent, and training them with CTDE methods allows for scaling to large systems.
Framing it as a multi-agent problem allows it to scale and to outperform rule-based approaches, considered the state-of-the-art for solving these problems.

Part \ref{part:compet} tackles training agents to learn to cooperate when competing against another team.
This mixed cooperative-competitive setting requires to benefit from the methods used to train agents from one team to cooperate along with the training scenarios required to train an agent in a competitive environment.
This part is divided into two chapters. 
The former provides background on the competition setting, while the latter presents specific contributions where we trained teams to compete against each other.

Chapter \ref{ch:competition} addresses challenges when agents do not purely cooperate.
This is the case not only in competitive settings but also in general-sum ones.
This chapter presents different types of solutions that game theory provides because Nash equilibrium is not the single one.
Following this, we discuss historical methods for computing these solutions, from dynamic programming to planning methods.
The chapter concludes with the presentation of self-play and population training, two popular training scenarios for training agents in competitive or general-sum environments.

Chapter \ref{ch:2teams} presents a study to train one team of agents to compete against another.
This setting is a symmetric two-team Markov game where two teams composed of the same agents compete.



is the third background chapter focusing on the competition scenario.
The different types of solution in multi-agent system provided by game theory are reviewed.