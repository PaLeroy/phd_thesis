\chapter{The Deep Quality-Value family in Dec-POMDP}\label{ch:qvmix}
\begin{chapter_outline}

This chapter presents four value-based methods from the Deep Quality-Value (DQV) family for the Dec-POMDP framework.
After an introduction in Section \ref{sec:ch4_intro}, we formalise these methods in Section \ref{sec:ch4_methods}.
The experimental setup to evaluate them is presented in Section \ref{sec:ch4_experiments}, followed by the corresponding results in Section \ref{sec:ch4_results}.
This chapter ends with discussions and conclusions in Section \ref{sec:ch4_conclusion}.

This chapter is an adapted version of the publication \citep{leroy2020qvmix} \textit{QVMix and QVMix-Max: extending the deep quality-value family of algorithms to cooperative multi-agent reinforcement learning}, P. Leroy, D. Ernst, P. Geurts, G. Louppe, J. Pisane, and M. Sabatelli. AAAI-21 Workshop on Reinforcement Learning in Games, 2021.

\end{chapter_outline}


\section{Introduction} \label{sec:ch4_intro}

This chapter introduces four methods to learn to cooperate in multi-agent reinforcement learning (MARL) in a decentralised partially observable Markov decision process (Dec-POMDP).
The latter is defined in Section \ref{sec:ch3_decpomdp}.
These four methods are adapted from the Deep Quality-Value (DQV) family \citep{sabatelli2020deep}.
The key idea of DQV methods is to jointly learn an approximation of the state-value function $V$ alongside an approximation of the state-action value function $Q$.
They have proven to significantly outperform popular algorithms such as DQN \cite{Mnih2015} and DDQN \cite{van2016deep} in single-agent reinforcement learning (SARL), both defined in Section \ref{sec:ch2_value_based_methods}.

The four methods introduced in this chapter follow two modes of training and execution in a Dec-POMDP, introduced in Section \ref{sec:ch3_intro}.
Two methods are designed for the decentralised mode, namely IQV and IQV-Max.
The other two are for the centralised training with a decentralised execution (CTDE) mode. 
They are called QVMix and QVMix-Max.

These four algorithms are tested in the Dec-POMDP framework, specifically with the StarCraft Multi-Agent Challenge (SMAC) suite of environments, introduced in Section \ref{fig:ch3_smac}.
They are compared with IQL, QMIX and MAVEN, which are three value-based methods in multi-agent reinforcement learning defined in Section \ref{sec:ch3_value}.

Finally, the contributions of \citep{leroy2020qvmix} presented in this chapter can be divided into three parts.
The first is the generalisation of the DQV family of algorithms to cooperative MARL problems, their performance in the decentralised mode and their fundamental limitations.
The second is the introduction of two methods, QVMix and QVMix-Max.
Both combine the original benefits of the DQV algorithms with CTDE, resulting in a better performance than state-of-the-art techniques of their time.
The third is to link the performance of QVMix, to the overestimation bias of the $Q$ function that characterises model-free RL algorithms.

\section{Methods} \label{sec:ch4_methods} 

As introduced, the work presented in this chapter revolves around the Deep Quality-Value (DQV) family of DRL algorithms \cite{sabatelli2018deepQV, sabatelli2020deep}.
The core idea of these SARL techniques is to learn an approximation of the state-value function $V$ alongside an approximation of the state-action value function $Q$.
The DQV algorithms generalise the tabular RL algorithms called QV($\lambda$) learning \citep{wiering2005qv, wiering2009qv} with neural networks as function approximators.
Learning a joint approximation of the $V$ function and the $Q$ function can be done in two different ways, both of them requiring two different neural networks for learning $Q(s, u;\theta)\approx Q^{\pi^*}(s, u)$ and $V(s;\phi)\approx V^{\pi^*}(s)$.
\todo{read matthia thesis to add more}
DQV learns an approximation of the $V$ function by minimising the loss defined in Equation \ref{eq:ch4_dqv_v_update} while the loss defined in Equation \ref{eq:ch4_dqv_q_update} is minimised for learning the $Q$ function.

\begin{equation}
    \mathcal{L}(\phi) = \mathbb{E}_{\langle s_{t}, u_{t},r_{t},s_{t+1}\rangle\sim B} \bigg[\big(r_{t} + \gamma V(s_{t+1}; \phi') - V(s_{t}; \phi)\big)^{2}\bigg]
    \label{eq:ch4_dqv_v_update}
\end{equation}
    
\begin{equation}
    \mathcal{L}(\theta) = \mathbb{E}_{\langle s_{t}, u_{t},r_{t},s_{t+1}\rangle\sim B} \bigg[\big(r_{t} + \gamma V(s_{t+1}; \phi')  - Q(s_{t}, u_{t}; \theta)\big)^{2}\bigg]
\label{eq:ch4_dqv_q_update}
\end{equation}
    
DQV-Max learns the $Q$ function with the same loss defined in Equation \eqref{eq:ch4_dqv_q_update} but learns the $V$ function with a different loss defined in Equation \ref{eq:ch4_dqv_max_v}.
    
\begin{equation}
        \mathcal{L}(\phi) = \mathbb{E}_{\langle s_{t}, u_{t},r_{t},s_{t+1}\rangle\sim B} \bigg[\big(r_{t} + \gamma \: \underset{u\in \mathcal{U}}{\max}\: Q(s_{t+1}, u; \theta') - V(s_{t}; \phi)\big)^{2}\bigg].
        \label{eq:ch4_dqv_max_v}
\end{equation}

As in DQN, the replay buffer $B$ stores transitions $\langle s_{t},u_{t},r_{t},s_{t+1}\rangle$ from which batches of transitions are sampled to update the networks.
The target network $\theta'$ and $\phi'$ are a copies of $\theta$ and $\phi$ updated periodically.

The extension to MARL methods is straightforward.
For the decentralised mode, this leads to IQV and IQV-Max using the DQV and DQV-Max update rules and following the same principles as IQL.
For the CTDE mode, the approximation of $Q$ is the same as in QMIX, using the same network architecture.
However, it is now updated with the loss defined in Equation \ref{eq:ch4_dqv_q_update}.
In both QVMix and QVMix-Max, the $V$ network has the same architecture as the $Q$ network of QMIX, except that actions are not considered.
The architecture is presented in Figure \ref{fig:ch4_qvmix}.
In QVMix, this network is updated with the loss defined in Equation \ref{eq:ch4_dqv_v_update} while with the loss defined in Equation \ref{eq:ch4_dqv_max_v} for QVMix-Max. 
As in QMIX, $B$ stores sequences of contiguous transitions instead of single transitions to train recurrent neural networks.

\begin{figure}
\centering
\input{tex_thesis/tikz/ch4/qvmix}
\caption{Architecture of the $V_{mix}$ network.}
\label{fig:ch4_qvmix}
\end{figure}

\section{Experiments} \label{sec:ch4_experiments}

Our experiments test seven methods.
The four defined in this chapter, QVMix, QVMix-Max, IQV and IQV-Max and the three forming the state of the art at that time, QMIX, MAVEN and IQL, all three defined in Chapter \ref{ch:cooperation}.
As a test-bed, we use SMAC and evaluate the methods on eight different scenarios (also called maps): \texttt{3m}, \texttt{8m}, \texttt{so\_many\_baneling}, \texttt{2m\_vs\_1z}, \texttt{MMM}, \texttt{2s3z}, \texttt{3s5z} and \texttt{3s\_vs\_3z}. 
The goal is to maximise the reward, achieved by reducing each opponent team unit's health to zero, called a win.
We refer the reader to Section \ref{sec:ch3_smac} for the details on SMAC and a more in-depth explanation of the differences between map configurations.
It is worth noting that the maps chosen for our experiments differ in complexity. 

Each algorithm is trained on every map ten times, and each neural network is trained from scratch.
Networks are trained for either $5m$ or $10m$ timesteps depending on the considered map and the time required to achieve convergence.
Every $20000$ timestep, the parameters of the networks are saved to perform $24$ testing episodes.

Each algorithm uses the same type of hyper-parameters to compare the tested algorithms as fairly as possible.
Specifically, we refer to the authors of QMIX, MAVEN, and IQL to determine the hyper-parameters set and keep the same values for QVMix, QVMix-Max, IQV, and IQV-Max. 
For a more thorough presentation of all used hyper-parameters, we refer the reader to the open-sourced code\footnote{\url{https://github.com/PaLeroy/QVMix}}.
As is common practice within the literature, the individual networks' parameters are shared among agents to improve the algorithms' learning speed.
This means a single individual $Q_a$ network is used for all agents.
Moreover, a one-hot encoding of the agent id ($\{1,..,n\}$) is added to the observation space to allow agents to have different policies.

\section{Results} \label{sec:ch4_results}

\begin{table*}
    \centering
    \input{tex_thesis/figures/ch4/table_mean}
    \caption{
    Means of win rates achieved in eight scenarios at the end of training by QMIX, MAVEN, QVMix, QVMix-Max, IQL, IQV, and IQVMax. 
    In the first four scenarios, \texttt{3m}, \texttt{8m}, \texttt{so\_many\_baneling} and \texttt{2m\_vs\_1z}, it is measured after $5$ millions training timesteps.
    In the last four, \texttt{MMM}, \texttt{2s3z}, \texttt{3s5z} and \texttt{3s\_vs\_3z} it is measured after $10$ millions training timesteps.
    We report the best and second-best means in the green and yellow cells. When results are equivalent, the cells report the fastest and second-fastest method that reaches a win-rate of $100\%$ as shown in Figure \ref{fig:all_win_curves}.}
    \label{tab:main_results}
\end{table*}

We report the results of experiments in two different ways.
We start by first analysing the win rate of each tested method and then also investigate the quality of the value functions learned by all algorithms.

The means of win rate for each map and algorithm are reported in Table \ref{tab:main_results}. 
Suppose the algorithms perform equally in terms of overall performance, meaning the average win rate is the same. 
In that case, we consider the one which significantly converges the fastest to be the best-performing algorithm.
Please note that reporting the win rate of an episode is a good indicator of the quality of an agent's learned policy since, as introduced in the previous section, a win directly corresponds to the best achievable sum of rewards an agent can receive.

We start by observing the differences in terms of performance between methods for the decentralised mode (IQL, IQV and IQV-Max) and their respective CTDE extensions (QMIX, QVMix and QVMix-Max) and MAVEN.
As one might expect, we can see from the results reported in Figure \ref{fig:ch4_2m1zwin} that methods for the decentralised mode converge slowly when compared to their CTDE counterparts on the considered \texttt{2\_vs\_1z} map.
This is particularly interesting since it shows that the DQV family of algorithms can be successfully adapted to MARL, both in a decentralised training set-up and in a CTDE one.
However, these results are challenged once the number of agents in the maps increases: examples of such maps are \texttt{so\_many\_baneling}, \texttt{MMM} or \texttt{3s5z}.
The performance of decentralised methods starts to drop, highlighting that CTDE methods are required once the complexity of the training scenario increases, as is clearly reported both in Table \ref{tab:main_results} and in Figure \ref{fig:all_win_curves}, where the evolution of the win rate of each algorithm on every tested map is presented.

\begin{figure}
\centering
\includegraphics[width=.8\linewidth]{tex_thesis/figures/ch4/2m1zall.pdf}
\caption{Mean of win rates achieved in the \texttt{2m\_vs\_1z} map by QVMix, QVMix-Max, QMIX, MAVEN, IQV, IQVMax and IQL. The error band is proportional to the variance of the measure. We can observe that all CTDE methods result in faster training than decentralised ones. All four novel algorithms based on the DQV family of algorithms can be successfully used in cooperative MARL.}
\label{fig:ch4_2m1zwin}
\end{figure}

\begin{figure*}
\centering
\includegraphics[width=.95\linewidth]{tex_thesis/figures/ch4/all_win.pdf}
\caption{Means of win-rates achieved by QVMix, QVMix-Max, QMIX, MAVEN, IQV, IQVMax and IQL in eight scenarios. Top to bottom, left to right, the scenarios are \texttt{3m}, \texttt{8m}, \texttt{so\_many\_baneling}, \texttt{2m\_vs\_1z}, \texttt{MMM}, \texttt{2s3z}, \texttt{3s5z} and \texttt{3s\_vs\_3z}. The error band is proportional to the variance of win-rates.}
\label{fig:all_win_curves}
\end{figure*}

Therefore, directing attention to CTDE methods, we observe that only QVMix and QVMix-Max perform as well as QMIX and MAVEN in most of the eight maps.
When we consider the \texttt{MMM}, \texttt{3m}, \texttt{2m\_vs\_1z} and the \texttt{so\_many\_baneling} maps, we can observe that there is no significant difference between the performance that is obtained by our algorithms and that of QMIX and MAVEN. 
All methods converge towards the best possible winning rate and, in terms of convergence speed, perform closely.

However, when considering the \texttt{2s3z}, \texttt{3s5z} and \texttt{8m} maps, we observe that the performance of QVMix results in even faster learning. 
Of greater interest, when looking at the results obtained on the \texttt{3s5z} map, QVMix is the only algorithm approaching the best possible win rate.
It is also worth noting that the performance of QVMix-Max is always competitive with the one obtained by QVMix, QMIX and MAVEN.
These results are not surprising since a similar performance was observed when DQV-Max was tested in a SARL set-up \cite{sabatelli2020deep}.

\begin{figure*}
\centering
\includegraphics[width=.95\linewidth]{tex_thesis/figures/ch4/2m1z3s5zQ.pdf}
\caption{$Q$ values obtained and estimated when training QVMix, QVMix-Max, MAVEN and QMIX. Dash-dotted lines represent the obtained $Q$ values while solid lines represent the estimated ones.}
\label{fig:exp_plots_overestim:q_best_worse}
\end{figure*}

To understand the reasons behind why QVMix is the best performing algorithm overall, we analyse how well each method estimates the state-joint-action value function $Q_mix^{tot}(s_t, \mathbf{u_t})$. 
Since, in most maps, decentralised methods do not perform as well as CTDE methods, we restrict our analysis to CTDE algorithms only where their respective mixer networks give the estimated $Q(s_t, \mathbf{u_t})$.
Since the state space of the maps provided by the SMAC environment is not finite, it is unfortunately impossible to compute each state's real $Q(s_t, \mathbf{u_t})$.
To overcome this problem, we compute the discounted sum of rewards obtained with the current policy in each visited state during an episode and compare the results with the value function inferred from the $Q$ values estimated by the mixer network for these states.
The closer the estimates are to the real $Q(s_t, \argmax_{\mathbf{u}}(Q(s_t,\mathbf{u})))$, the more accurate the learned value function is.

For this experiment, we selected two different maps: the \texttt{2m\_vs\_1z} map, which corresponds to the map on which the best results have been achieved by all methods at the same time, and the \texttt{3s5z} map, which on the other hand, is the map on which QVMix performed less well.
In Figure \ref{fig:exp_plots_overestim:q_best_worse} we report the averaged estimated $Q$ values, represented by the solid lines, and the actual discounted sum of rewards, represented by the dash-dotted lines.
All are computed for each visited state at testing time.
In both scenarios, we can observe that the $Q$ values estimated by QMIX and MAVEN suffer from the overestimation bias of the $Q$ function, while this is not the case for QVMix and QVMix-Max.
We, therefore, justify the better quality of QVMix and QVMix-Max policies by a better approximation of the $Q$ functions.
However, further work will be required to understand this phenomenon in more detail.


\section{Discussion and future work} \label{sec:ch4_conclusion}

In this chapter, we introduced four new value-based methods that can be used to train a team of agents in a Dec-POMDP.
Two of our methods are designed for a fully decentralised execution training mode (IQV and IQVMax), while two are dedicated to centralised training with decentralised execution (QVMix and QVMix-Max).
We compared our algorithms with three methods taken from the literature and used the StarCraft Multi-Agent Challenge as a benchmark. 
We have shown that QVMix and QVMix-Max achieve the same results as popular state-of-the-art techniques (QMIX and MAVEN) and that on some of the maps, QVMix can result in faster and better learning.
We suggest that this better performance can be related to the fact that QVMix seems to suffer less from the overestimation bias of the $Q$ function.
In future work, it would be interesting to analyze each agent's behaviour and study the impact of the value function in the optimisation procedure.
Furthermore, we aim to find the best possible set of hyper-parameters of each algorithm so that their performance can be exploited even further and more fairly.

Restrospective: