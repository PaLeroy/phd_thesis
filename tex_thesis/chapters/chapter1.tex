\chapter{Introduction}\label{ch:introduction}

We all make decisions.
A decision is a choice among possibilities.
Consider yourself driving a car and arriving at a crossroads as an example.
In front of you, there is a traffic sign.
You predict it is a stop sign based on your perception and knowledge.
This decision involves reasoning, even if it is instantaneous.
But this decision has no impact on the car.
As suggested by the traffic sign, you then decide to take the action of stopping it.
This second decision, based on the first one, involves again reasoning but now impacts your environment.
Indeed, you push on the brakes, and the car decelerates until it stops.
This highlights one difference between decisions, which we differentiate as predicting or acting.
Their impact can often distinguish them: predicting doesn't affect our environment, while acting usually does.

Decisions are based on what we learned.
You studied the traffic signs to know it was a stop one.
You practised your driving skills to obtain your driver's license by trial and error, but hopefully also with the help of a supervisor.
You did acquire these skills by learning.
We learn by supervision, having examples of decisions and feedback to highlight whether some were right or wrong.
But we also learn by interaction, testing our environment.
Anyone remembers a child throwing objects from the table and learning what gravity is.
"The idea that we learn by interacting with our environment is probably the first to occur to us when we think about the nature of learning" \citep{sutton2018reinforcement}.
This highlights that you learn by yourself but also with the help of others.
Nevertheless, this manuscript is not about how humans make decisions. 
It is about how computers can learn to make decisions.
When we say computer makes decisions, we are talking about programs built to process some inputs to provide outputs.

When a computer makes decisions, it is called artificial intelligence (AI).
But AI is not only about computers making decisions.
Nowadays, AI has a lot of definitions.
\cite{russel2010} identify eight definitions to propose a classification of AI into four approaches.
They suggest that AI is to make computers either think like humans, think rationally, act like humans or act rationally.
Computers thinking like humans usually refers to the field of cognitive sciences where theories of the human mind are built.
Computers thinking rationally refers to the field of logic based on knowledge representation inspired by the syllogisms invented by Aristotle.
This manuscript does not address these two types of AI.
Indeed, we are interested in computers acting to solve problems.
Acting can be defined as making decisions, and \cite{russel2010} consider acting as anything related to the behaviour.

When considering AI acting like humans, a well-known idea coming up is the Turing test.
An AI succeeds the Turing test if, during a written conversation with it, the human cannot tell whether its interlocutor is an AI.
This test has been extended to include video signals or physical interaction to test more complex AI.
However, many researchers did not focus on creating acting AI succeeding this test.
The following self-explanatory quote may convince many: "Aeronautics is not defined as the field of making machines that fly so exactly like pigeons that they can fool even other pigeons"\citep{russel2010}.
This brings us to the fourth category of AI definitions, the one of interest in this manuscript: AI acting rationally.
Rationality means mathematics and engineering here, such as in the logic approach previously described.
Therefore, these types of AI are built to take decisions to achieve a predefined goal.
An agent is defined as anything taking decisions.
A rational agent acts to achieve the best-expected outcomes of its objective.

This manuscript is about AI, specifically about agents acting rationally, but it is also about learning, and learning has not yet been addressed.
And this is because it is possible to act rationally without learning.
It is possible to solve a Sudoku game by sequentially enumerating all cells' available numbers and assigning a number to cells that end with only one possible number.
This does not involve learning anything but only applying the game rules ingenuously.
Many problems can be solved without learning, but some may be challenging.
For example, a computer can play Chess by enumerating all the possibilities from a given board situation.
But the number of possible games of Chess is $~10^{120}$ \citep{shannon1950xxii}, and enumerating all of them is intractable.
However, creating an AI that plays Chess is possible if you enumerate only the games of interest, and do it in parallel to make it fast.
This is how DeepBlue \citep{campbell2002deep} plays Chess at the level of the best human players without learning.
Even though AI can solve very complex problems by ingeniously exploiting predefined rules, the complexity of some problems leads to the development of AI that can learn to make decisions.

When there is learning in AI, we call it machine learning (ML): "Machine learning is a subset of AI that learns to make decisions by fitting mathematical models to observed data"~\citep{prince2023understanding}.
ML is divided into different areas based on the available data and the algorithm's goal.
For example, an AI can learn to predict the type of a traffic sign based on its picture.
To achieve this, a model can be built with the help of a dataset of pictures and their corresponding labels.
The model is a mathematical function that maps a picture, a collection of pixel values, to a type of traffic sign.
The model is initially bad and improves while training. 
It learns!
During the training phase, the system predicts the types of images from the dataset.
By using actual and predicted types of traffic signs, the model is refined to improve it.
Such a solution is categorised as supervised learning (SL) because the labels supervise the learning.
Another area of ML is unsupervised learning when labels are not provided with the data.
Indeed, the lack of labels induces the absence of supervision, but the model can still learn to understand the image structure instead of learning to predict its label.
Removing part of the image and training the model to predict the missing pixel is an example of unsupervised learning called inpainting.
Unsupervised learning is used to pre-train the model by figuring out the data before providing some labels to learn a given task with SL.

Going back in time, at the beginning of this introduction, we first dissociated decisions by their impact on the environment.
Learning to predict something via supervised learning allows one to make decisions that impact or not the environment.
Indeed, it is possible to train a model that predicts the strength to apply to your car pedals based on a camera in the car.
We also considered Chess.
With a collection of chess games, it would also be possible to predict the best move based on the ones seen in the dataset.
Aside from the complexity of acquiring such datasets, these approaches do not allow AI to learn to make decisions but only to imitate them.
Doesn't this remind you of the acting humanly approach of AI defined previously?
If the dataset comprises purely rational decisions, we can consider the approach as learning to act rationally.
Nevertheless, instead of learning to imitate decisions, we need a systematic approach to train an agent to make rational decisions.
This can be done by providing a numerical signal after each decision, evaluating how good the decision is given a defined goal.
Such an approach exists and is called reinforcement learning (RL).
This is a third area of machine learning, and most of the time, any ML problem is classified as one of the three approaches presented: supervised, unsupervised or reinforcement learning.

Let us now describe reinforcement learning.
In RL, an agent acts and receives a reward after taking action.
The reward is the action's outcome, a numerical feedback.
As the goal is to act rationally, the agent is trained to find the strategy that provides the best possible reward.
This strategy is called the optimal policy.
The agent learns it by trial and error, typically updating its policy based on the received reward after each action.
Back to our example, when not stopping at a stop sign at a crossroad, you can cause an accident, an excellent example of a negative reward.
But whether you break or not, your environment is evolving.
Indeed, in addition to receiving rewards based on the actions taken by the agent, the agent's environment can also evolve.
We then say that the state of the environment evolves based on the action.
The agent is part of the environment but interacts with it, somehow defined inside and outside of it.
Nevertheless, this dynamic nature of the environment adds a new dimension to the learning process because the agent needs to learn to make a sequence of optimal decisions.
Note that it is possible to design an RL problem without evolution.

Sudoku and Chess can be solved without learning, but the AI needs to know the game's rules.
In RL, this is usually not the case.
The agent does not know how an action will affect its state nor the reward it will receive.
It just receives the reward after taking an action and learns to maximise it.
One of the challenges of RL is that agents face the exploration-exploitation dilemma.
They must balance trying out new actions to discover potentially better strategies (exploration) and sticking to actions that have yielded high rewards in the past (exploitation).
Another challenge is the environment, which can also involve delayed rewards, where the consequences of an action may not be immediate, typically because a sequence of precise actions is required to obtain the reward. 
This delay makes it more challenging for agents who receive a reward and understand whether it was for a previous or last action.
This is referred to as the credit assignment challenge.

Many applications of reinforcement learning exist.
It can be applied in healthcare, typically to decide treatment dosage through time \citep{miotto2018deep}.
A recommender system is also an example of a reinforcement learning application \citep{mcinerney2018explore}.
Robotics is another example where RL learns to control a robot, such as learning a robotic hand to solve Rubik's cube \citep{akkaya2019solving}.
\todo{Chip design cite{A graph placement methodology for fast chip design}}
Finally, games have been a testbed for reinforcement learning for a long time.
Some significant breakthroughs have been achieved thanks to games, such as attaining human performance in Atari games \citep{Mnih2015}.
They offer the best environment for trials and errors as they can be easily simulated.
There are many other applications, but returning to our example, we did not consider one important parameter.
When riding your car, you are not alone.
Other people are also sharing the roads, and your decisions are also based on them, on their behaviour.

When there are multiple agents in the environment, many things change.
We need to consider how these agents influence the environment.
There are several types of dynamics in a multi-agent system.
Agents can take actions simultaneously or sequentially.
For example, everyone makes decisions simultaneously on the road, while Chess is a turn-based game where agents' decisions are made one after the other.
In these two examples, the number of agents is also different.
In Chess, there are only two agents, while on the road, there can be a lot.
These two examples illustrate the range of problems with multi-agent systems, from a game with two agents to a real-world environment with an unknown number of agents.
One additional and crucial distinction that should be made in multi-agent environments is that some agents may not learn.

Multi-agent reinforcement learning (MARL) extends single-agent reinforcement learning (SARL) when multiple agents learn in the environment.
Suppose only one agent is learning in an environment composed of several ones.
The other agents then have a stationary policy, which does not evolve through time.
Thus, the agent can be trained with SARL because the other agents are simply part of its environment since they always act the same.
However, when several agents learn, their policies change over time.
From the point of view of each agent, the environment dynamic is constantly changing.
This induces non-stationarity in the learning process, known as a moving target problem, and is a first challenge in MARL.
Typically, in a rock-paper-scissor game, if one always chooses rock, one may always choose paper.
But if the first changes to scissors, the second may finally change to rock.
This cycle can last forever, and one needs to decide when to stop.
Finding an equilibrium where strategies stop evolving is indeed a challenging solution.

A second challenge in MARL is to determine the optimality of policies.
In SARL, it is straightforward: the agent must achieve a high sum of rewards, and when it finds the policy achieving the maximum sum of rewards, it stops.
But how can this be posed in a multi-agent environment when several agents want to maximise their reward?
The solution of finding an equilibrium is achieved when agents do not want to change their policy.
How to define this equilibrium and whether there are several of them is a dedicated research problem in MARL.
Moreover, finding the equilibrium providing the maximum sum of rewards can also be challenging.

The third challenge is the number of agents.
The environment's dynamic is often a function of every agent's action, leading to a combination of actions that scales exponentially with the number of agents.
Designing methods to train these agents that scale is challenging because it usually implies functions based on all these possibilities.

A fourth challenge is related to the credit assignment previously introduced.
In addition to understanding that a reward may be caused by a series of actions, a challenge in MARL is determining the actions of which agents caused the current reward.
Back to the example, if you stop because of a stop sign but get damaged by the one following you who did not stop, the corresponding negative reward is not your fault, but the faulty one is the car's driver behind you.
Other challenges exist, but these four highlight the additional challenges when extending to multi-agent reinforcement learning \cite{marl-book}.

Multi-agent reinforcement learning has been divided into three principal settings.
Each one is based on the goal of agents.
The first setting, cooperation, involves agents cooperating to achieve a common goal.
In the second setting, competition, agents compete to achieve an opposing goal, typically named the zero-sum setting.
The third setting is named general-sum and encompasses everything else.
Based on these settings, it is possible to frame the problem in a specific way, helping to solve it more easily.
Many questions remain when solving MARL, but we kept one.

To summarise, different AI definitions exist, and we focus on how a computer can learn to act rationally.
Learning is not mandatory to act rationally but might be required when complexity increases.
Learning in AI is called machine learning and is divided into three sub-areas: supervised or unsupervised learning and reinforcement learning.
Reinforcement learning allows agents to be trained based on a reward they obtain after making a decision.
When several agents learn together with RL, it is multi-agent reinforcement learning.
There are several types of MARL, cooperation, competition and general-sum, and we identified the main challenges irrespective of the three.
Finally, enfin, this manuscript presents contributions to multi-agent reinforcement.

This introduction may make it clear that we need to consider other learning agents when training one.
The reader should be confident that training an agent with a single-agent reinforcement learning method might not yield the best answer to these problems.
This research question has existed in the MARL community for quite some time, and we go beyond by providing insights into how we should consider other learning agents in an environment.
We focus on two specific settings: training a team to cooperate and training a team to compete against an opposing one.
In the cooperative setting, we present methods designed to train a team of agents to fulfil a cooperative task and how such methods can be applied to a real-world application.
When training a team in an adversarial scenario, we can use the same methods but, again, by considering that there are opponents.
This highlights the two main parts of the manuscript, each providing background to provide perspective on the novelties of the presented work.
Before diving into the details of these two settings, we provide the necessary background and notations of reinforcement learning.
In the following sections, we provide a more detailed outline of this manuscript, followed by a disclaimer on the publications at the foundations of this manuscript and, finally, the context in which this work has been conducted.

\section{Outline}
\label{sec:ch1_outline}
This manuscript is divided into four parts.
The necessary background is presented in Part~\ref{part:background}, providing an overview of reinforcement learning.
In Part \ref{part:coop}, we present works related to cooperation, one of the multi-agent settings, while Part \ref{part:compet} extends these topics to a specific general-sum framework where teams compete.
Finally, Part \ref{part:conclusion} concludes this manuscript, retrospectively reviewing the contributions presented in this manuscript.

\begin{description}
    \item [Part \ref{part:background}] Background:
    
    Chapter~\ref{ch:background} is the single chapter of Part \ref{part:background} presenting the basic multi-agent reinforcement learning framework and details its different settings and challenges.
    More importantly, this chapter presents the foundations of MARL issued from single-agent reinforcement learning.
    For those unfamiliar with reinforcement learning, this chapter is a must-read, and we refer the reader to numerous additional works if more details are required.
    In contrast, these concepts should be well known for the familiar ones, and Chapter~\ref{ch:background} serves as a notation introduction.

    \item [Part \ref{part:coop}] Learn to cooperate:
    
    Chapter~\ref{ch:cooperation} is the first of this second part and provides the necessary material to understand the challenges of cooperation in multi-agent reinforcement learning.
    It is a mix of background introduction and literature review, as this chapter introduces many concepts and methods from the literature.
    
    In the second chapter, Chapter~\ref{ch:qvmix}, we present four methods based on the Deep-Quality Value family of algorithms designed to train cooperating agents.
    
    Chapter~\ref{ch:impmarl} is the third and last chapter and introduces infrastructure management planning, a real-world application that can be tackled in this cooperative framework.
    In this application, inspections, repairs, and/or retrofits should be timely planned to control the risk of potential system failures.
    
    \item [Part \ref{part:compet}]  Cooperate against an opposing team:
    
    Chapter~\ref{ch:competition} is the first one and presenting how competition is classically modelled and solved in multi-agent reinforcement learning.
    It serves the same purpose of Chapter~\ref{ch:cooperation} and is alike, a mix between background and literature review.
    
    Chapter~\ref{ch:2teams} presents a particular case of the general-sum settings where two teams compete and how to train them.

    \item [Part \ref{part:conclusion}] Conlusion:
    The last chapter of this manuscript, Chapter~\ref{ch:conclusion}, addresses the closing remarks.
\end{description}

\section{Publications}
\label{sec:ch1_publications}

This manuscript is built upon existing research, integrating insights from numerous cited sources.
Notably, three central chapters represent adaptations of specific peer-reviewed publications.

\begin{itemize}
\item Chapter~\ref{ch:qvmix} is an adapted version of the publication \textit{QVMix and QVMix-Max: extending the deep quality-value family of algorithms to cooperative multi-agent reinforcement learning}, P. Leroy, D. Ernst, P. Geurts, G. Louppe, J. Pisane, and M. Sabatelli. AAAI-21 Workshop on Reinforcement Learning in Games, 2021 \citep{leroy2020qvmix}.

\item Chapter~\ref{ch:impmarl} is an adapted version of the publication \textit{IMP-MARL: a suite of environments for large-scale infrastructure management planning via MARL}, P. Leroy, P. G. Morato, J. Pisane, A. Kolios, and D. Ernst. Thirty-seventh Conference on Neural Information Processing Systems Datasets and Benchmarks Track, 2023 \citep{leroy2023impmarl}.

\item Chapter~\ref{ch:2teams} is an adapted version of the publication \textit{Value-based CTDE methods in symmetric two-team Markov game: from cooperation to team competition}, P. Leroy, J. Pisane, and D. Ernst. Deep Reinforcement Learning Workshop NeurIPS, 2022 \citep{leroy2022twoteam}.
\end{itemize}


\section{Context}
\label{sec:ch1_context}
To provide a backstory, part of the work presented in this manuscript has been developed during the IRIS project with an industrial consortium of Belgian companies.
IRIS stands for Intelligent Recognition Information System, and the partners were John Cockerill Defense, ACIC, Multitel, Belgian Royal Military Academy and the University of Li{`e}ge.
The IRIS project aims to enhance surveillance capabilities by integrating machine learning.
Through detection, recognition, and behaviour analysis modules, IRIS seeks to empower operators with timely, actionable information to make optimal decisions in both military and civilian contexts, impacting the safety of individuals, sensitive sites, and equipment integrity.
Specifically, we explored multi-agent reinforcement learning to provide decision-aid to operators in the context of military missions.
In short, the IRIS project is a use case of our studies on training a team against an opposing team to learn how military assets should cooperate against a team of the same asset.
%We acknowledge the Walloon Region's financial support in this project's context.

% The other part of the work was done in collaboration with Pablo G. Morato, where we tackled a different real-world problem: infrastructure management planning.
% One of the challenges for this application is to scale the management when the infrastructure is composed of many parts.
% As presented in Chapter \ref{ch:impmarl}, the problem of minimising a system's risk and maintenance cost can become complex when the number of components increases.
% Applying multi-agent reinforcement learning allowed us to scale by decomposing the decisions.
